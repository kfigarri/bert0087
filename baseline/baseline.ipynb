{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "from pydantic import BaseModel, computed_field\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "dataset_name = \"contractnli\"\n",
    "vectorstore_path = \"./vectorstore/faiss_store_gte_base\"\n",
    "test_file = f\"../data/benchmarks/{dataset_name}.json\"\n",
    "result_file = f\"../data/results/qa_results.json\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"thenlper/gte-base\") \n",
    "\n",
    "# sentence-transformers/all-MiniLM-L6-v2\n",
    "# Linq-AI-Research/Linq-Embed-Mistral\n",
    "# thenlper/gte-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3307 document chunks with spans.\n",
      "FAISS vector store saved locally at ./vectorstore/faiss_store_gte_base.\n"
     ]
    }
   ],
   "source": [
    "def load_documents_with_spans(directory: str, chunk_size: int = 1000, chunk_overlap: int = 0):\n",
    "    \"\"\"\n",
    "    Loads .txt files from a directory, splits each document's text into chunks using\n",
    "    RecursiveCharacterTextSplitter, computes the span (start, end) for each chunk, and\n",
    "    returns a list of Document objects with metadata (including filename, source, and span).\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    # Initialize the splitter with the desired separators and parameters.\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"!\", \"?\", \".\", \":\", \";\", \",\", \" \", \"\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "        strip_whitespace=False,\n",
    "    )\n",
    "    \n",
    "    # Process each .txt file in the directory.\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Split text into chunks.\n",
    "            text_splits = splitter.split_text(text)\n",
    "            \n",
    "            # Verify that the chunks concatenate to the original text.\n",
    "            assert \"\".join(text_splits) == text, \"Concatenated splits do not match the original text.\"\n",
    "            \n",
    "            # Compute spans and create Document objects.\n",
    "            prev_index = 0\n",
    "            for i, chunk_text in enumerate(text_splits):\n",
    "                span = (prev_index, prev_index + len(chunk_text))\n",
    "                prev_index += len(chunk_text)\n",
    "                doc = Document(\n",
    "                    page_content=chunk_text,\n",
    "                    metadata={\n",
    "                        \"filename\": filename,\n",
    "                        \"filepath\": f\"{dataset_name}/{filename}\",\n",
    "                        \"span\": span,  # Stores the (start, end) positions of the chunk.\n",
    "                        \"id\": f\"{filename}_chunk_{i}\"\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Update this to the folder where your ContractNLI .txt files reside.\n",
    "directory_path = f\"./data/corpus/{dataset_name}\"\n",
    "\n",
    "# Load the documents, splitting each into chunks with span metadata.\n",
    "documents = load_documents_with_spans(directory_path, chunk_size=500, chunk_overlap=0)\n",
    "print(f\"Loaded {len(documents)} document chunks with spans.\")\n",
    "\n",
    "# Build the FAISS vector store using the list of Document objects.\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Save the FAISS vector store locally for later retrieval.\n",
    "vectorstore.save_local(vectorstore_path)\n",
    "print(f\"FAISS vector store saved locally at {vectorstore_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted the FAISS vector store at: ./vectorstore/faiss_store_gte_base\n"
     ]
    }
   ],
   "source": [
    "# import shutil\n",
    "\n",
    "# # Check if the directory exists\n",
    "# if os.path.exists(vectorstore_path):\n",
    "#     shutil.rmtree(vectorstore_path)\n",
    "#     print(f\"Deleted the FAISS vector store at: {vectorstore_path}\")\n",
    "# else:\n",
    "#     print(f\"No FAISS vector store found at: {vectorstore_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hk/j9r7jggx4dxgt8gmzj_c2z080000gn/T/ipykernel_7045/444561711.py:132: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  \"ground_truth\": [gt_snippet.dict() for gt_snippet in gt.snippets],\n",
      "/var/folders/hk/j9r7jggx4dxgt8gmzj_c2z080000gn/T/ipykernel_7045/444561711.py:133: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  \"retrieved\": [snippet.dict() for snippet in retrieved_snippets]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA results saved to ./data/qa_results.json.\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Define Data Models\n",
    "#############################\n",
    "\n",
    "class QASnippet(BaseModel):\n",
    "    file_path: str\n",
    "    span: Tuple[int, int]\n",
    "    answer: str\n",
    "\n",
    "class QAGroundTruth(BaseModel):\n",
    "    query: str\n",
    "    snippets: List[QASnippet]\n",
    "\n",
    "class RetrievedSnippet(BaseModel):\n",
    "    file_path: str\n",
    "    span: Tuple[int, int]\n",
    "    text: str      # Retrieved text content from the FAISS vectorstore\n",
    "    score: float   # Relevance score returned by similarity search\n",
    "\n",
    "class QAResult(BaseModel):\n",
    "    qa_gt: QAGroundTruth\n",
    "    retrieved_snippets: List[RetrievedSnippet]\n",
    "\n",
    "    @computed_field\n",
    "    @property\n",
    "    def precision(self) -> float:\n",
    "        total_retrieved_len = 0\n",
    "        relevant_retrieved_len = 0\n",
    "        for snippet in self.retrieved_snippets:\n",
    "            total_retrieved_len += snippet.span[1] - snippet.span[0]\n",
    "            # Compare with each ground-truth snippet (they are guaranteed not to overlap)\n",
    "            for gt_snippet in self.qa_gt.snippets:\n",
    "                if snippet.file_path == gt_snippet.file_path:\n",
    "                    common_min = max(snippet.span[0], gt_snippet.span[0])\n",
    "                    common_max = min(snippet.span[1], gt_snippet.span[1])\n",
    "                    if common_max > common_min:\n",
    "                        relevant_retrieved_len += common_max - common_min\n",
    "        if total_retrieved_len == 0:\n",
    "            return 0\n",
    "        return relevant_retrieved_len / total_retrieved_len\n",
    "\n",
    "    @computed_field\n",
    "    @property\n",
    "    def recall(self) -> float:\n",
    "        total_relevant_len = 0\n",
    "        relevant_retrieved_len = 0\n",
    "        for gt_snippet in self.qa_gt.snippets:\n",
    "            total_relevant_len += gt_snippet.span[1] - gt_snippet.span[0]\n",
    "            for snippet in self.retrieved_snippets:\n",
    "                if snippet.file_path == gt_snippet.file_path:\n",
    "                    common_min = max(snippet.span[0], gt_snippet.span[0])\n",
    "                    common_max = min(snippet.span[1], gt_snippet.span[1])\n",
    "                    if common_max > common_min:\n",
    "                        relevant_retrieved_len += common_max - common_min\n",
    "        if total_relevant_len == 0:\n",
    "            return 0\n",
    "        return relevant_retrieved_len / total_relevant_len\n",
    "\n",
    "#############################\n",
    "# Helper Functions\n",
    "#############################\n",
    "\n",
    "def load_groundtruth(json_file_path: str) -> List[QAGroundTruth]:\n",
    "    \"\"\"\n",
    "    Loads the QA ground-truth data from a JSON file.\n",
    "    Expected JSON format:\n",
    "    {\n",
    "        \"tests\": [\n",
    "            {\n",
    "                \"query\": \"Your query...\",\n",
    "                \"snippets\": [\n",
    "                    {\n",
    "                        \"file_path\": \"path/to/file.txt\",\n",
    "                        \"span\": [start, end],\n",
    "                        \"answer\": \"The answer text...\"\n",
    "                    },\n",
    "                    ...\n",
    "                ]\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    groundtruth_tests = []\n",
    "    for test in data.get(\"tests\", []):\n",
    "        snippets = [QASnippet(**snippet) for snippet in test[\"snippets\"]]\n",
    "        groundtruth_tests.append(QAGroundTruth(query=test[\"query\"], snippets=snippets))\n",
    "    return groundtruth_tests\n",
    "\n",
    "def perform_retrieval(vectorstore: FAISS, query: str, k: int = 5) -> List[RetrievedSnippet]:\n",
    "    \"\"\"\n",
    "    Uses the FAISS vector store to perform a similarity search on the given query using\n",
    "    similarity_search_with_relevance_score. Converts the returned Document objects into \n",
    "    RetrievedSnippet instances using the metadata, and also stores the relevance score.\n",
    "    \"\"\"\n",
    "    # Retrieve a list of tuples: (Document, relevance_score)\n",
    "    docs_and_scores: List[Tuple[Document, float]] = vectorstore.similarity_search_with_relevance_scores(query, k=k)\n",
    "    retrieved = []\n",
    "    for doc, score in docs_and_scores:\n",
    "        # Retrieve file path and span from metadata.\n",
    "        file_path = doc.metadata.get(\"filepath\")\n",
    "        span = doc.metadata.get(\"span\", (0, len(doc.page_content)))\n",
    "        retrieved.append(RetrievedSnippet(file_path=file_path, span=span, text=doc.page_content, score=score))\n",
    "    return retrieved\n",
    "\n",
    "#############################\n",
    "# Main Execution\n",
    "#############################\n",
    "\n",
    "# 1. Load ground-truth data.\n",
    "groundtruth_tests = load_groundtruth(test_file)\n",
    "\n",
    "# 2. Load the FAISS vector store that was previously created.\n",
    "vectorstore = FAISS.load_local(vectorstore_path, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# 3. Evaluate retrieval performance for different k values.\n",
    "k_values = [1, 3, 5, 10]\n",
    "all_results = []\n",
    "\n",
    "for gt in groundtruth_tests:\n",
    "    for k in k_values:\n",
    "        retrieved_snippets = perform_retrieval(vectorstore, gt.query, k=k)\n",
    "        qa_result = QAResult(qa_gt=gt, retrieved_snippets=retrieved_snippets)\n",
    "        # Create a dictionary of results for this query and k.\n",
    "        result_dict = {\n",
    "            \"query\": gt.query,\n",
    "            \"k\": k,\n",
    "            \"precision\": qa_result.precision,\n",
    "            \"recall\": qa_result.recall,\n",
    "            \"ground_truth\": [gt_snippet.dict() for gt_snippet in gt.snippets],\n",
    "            \"retrieved\": [snippet.dict() for snippet in retrieved_snippets]\n",
    "        }\n",
    "        all_results.append(result_dict)\n",
    "\n",
    "# 4. Save the results as JSON.\n",
    "with open(result_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"QA results saved to {result_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision and Recall for each K:\n",
      "K = 1: Average Precision = 0.0299, Average Recall = 0.0355\n",
      "K = 3: Average Precision = 0.0213, Average Recall = 0.0565\n",
      "K = 5: Average Precision = 0.0191, Average Recall = 0.0693\n",
      "K = 10: Average Precision = 0.0174, Average Recall = 0.0989\n"
     ]
    }
   ],
   "source": [
    "with open(result_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Dictionary to collect precision and recall values per K.\n",
    "# The keys will be the K value and the values a list of (precision, recall) tuples.\n",
    "metrics_by_k = defaultdict(list)\n",
    "\n",
    "for item in results:\n",
    "    k = item.get(\"k\")\n",
    "    precision = item.get(\"precision\", 0)\n",
    "    recall = item.get(\"recall\", 0)\n",
    "    metrics_by_k[k].append((precision, recall))\n",
    "\n",
    "# Compute the average precision and recall for each K.\n",
    "avg_metrics = {}\n",
    "for k, metrics in metrics_by_k.items():\n",
    "    if metrics:\n",
    "        total_precision = sum(m[0] for m in metrics)\n",
    "        total_recall = sum(m[1] for m in metrics)\n",
    "        count = len(metrics)\n",
    "        avg_precision = total_precision / count\n",
    "        avg_recall = total_recall / count\n",
    "    else:\n",
    "        avg_precision = 0\n",
    "        avg_recall = 0\n",
    "    avg_metrics[k] = {\"avg_precision\": avg_precision, \"avg_recall\": avg_recall}\n",
    "\n",
    "# Print the results.\n",
    "print(\"Average Precision and Recall for each K:\")\n",
    "for k in sorted(avg_metrics.keys()):\n",
    "    metrics = avg_metrics[k]\n",
    "    print(f\"K = {k}: Average Precision = {metrics['avg_precision']:.4f}, Average Recall = {metrics['avg_recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"Consider EFCA's Non-Disclosure Agreement; Does the document permit the Receiving Party to create a copy of some Confidential Information under certain circumstances?\",\n",
       " 'k': 3,\n",
       " 'precision': 0.20080321285140562,\n",
       " 'recall': 1.0,\n",
       " 'ground_truth': [{'file_path': 'contractnli/EFCAConfidentialityAgreement.txt',\n",
       "   'span': [2459, 2609],\n",
       "   'answer': 'Copies or reproductions shall not be made except to the extent reasonably necessary and all copies made shall be the property of the disclosing party.'}],\n",
       " 'retrieved': [{'file_path': 'contractnli/EFCAConfidentialityAgreement.txt',\n",
       "   'span': [2280, 2609],\n",
       "   'text': '. EFCA shall ensure that disclosure of such Confidential Information is restricted to those employees or directors of EFCA and EFCA’s principals having the need to know the same. Copies or reproductions shall not be made except to the extent reasonably necessary and all copies made shall be the property of the disclosing party.',\n",
       "   'score': 0.8845122402835409},\n",
       "  {'file_path': 'contractnli/EFCAConfidentialityAgreement.txt',\n",
       "   'span': [4473, 4867],\n",
       "   'text': '\\n5. Confidentiality\\nEFCA agrees to keep the existence and nature of this Agreement confidential and not to use the same or the name of the disclosing party (or of any company in the Group of Companies of which the disclosing party forms part) in any publicity, advertisement or other disclosure with regard to this Agreement without the prior written consent of the disclosing party.\\n6. Notices',\n",
       "   'score': 0.8765695613145417},\n",
       "  {'file_path': 'contractnli/tpi-non-disclosure-agreement_1.txt',\n",
       "   'span': [0, 24],\n",
       "   'text': 'NON-DISCLOSURE AGREEMENT',\n",
       "   'score': 0.8719251155858989}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qidx = random_number = random.randint(1, len(results))\n",
    "results[qidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Consider the Non-Disclosure Agreement between IGC and LSE; Does the document allow the Receiving Party to share some Confidential Information with third parties, including consultants, agents, and professional advisors?',\n",
       " 'k': 5,\n",
       " 'precision': 0.0,\n",
       " 'recall': 0.0,\n",
       " 'ground_truth': [{'file_path': 'contractnli/IGC-Non-Disclosure-Agreement-LSE-Sample.txt',\n",
       "   'span': [4634, 4736],\n",
       "   'answer': 'Representative means employees, agents, officers, advisers and other representatives of the Recipient.'},\n",
       "  {'file_path': 'contractnli/IGC-Non-Disclosure-Agreement-LSE-Sample.txt',\n",
       "   'span': [6909, 7094],\n",
       "   'answer': \"The Recipient may disclose the Disclosing Party's Confidential Information to those of its Representatives who need to know this Confidential Information for the Purpose, provided that:\"}],\n",
       " 'retrieved': [{'file_path': 'contractnli/tpi-non-disclosure-agreement_1.txt',\n",
       "   'span': [0, 24],\n",
       "   'text': 'NON-DISCLOSURE AGREEMENT',\n",
       "   'score': 0.8735252839098877},\n",
       "  {'file_path': 'contractnli/ExcelerateStandardNDAFormat.txt',\n",
       "   'span': [0, 24],\n",
       "   'text': 'NON-DISCLOSURE AGREEMENT',\n",
       "   'score': 0.8735252839098877},\n",
       "  {'file_path': 'contractnli/SupplementOne-NDA.txt',\n",
       "   'span': [0, 24],\n",
       "   'text': 'NON-DISCLOSURE AGREEMENT',\n",
       "   'score': 0.8735252522997513},\n",
       "  {'file_path': 'contractnli/QuickBooks-NDA-template.txt',\n",
       "   'span': [0, 24],\n",
       "   'text': 'Non-disclosure Agreement',\n",
       "   'score': 0.8735252522997513},\n",
       "  {'file_path': 'contractnli/lti-two-way-cda-template.txt',\n",
       "   'span': [0, 24],\n",
       "   'text': 'NON-DISCLOSURE AGREEMENT',\n",
       "   'score': 0.8735252522997513}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qidx = random_number = random.randint(1, len(results))\n",
    "results[qidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"Consider Grindrod SA's Non-Disclosure Agreement; Does the document allow the Receiving Party to independently develop information that is similar to the Confidential Information?\",\n",
       " 'k': 10,\n",
       " 'precision': 0.16828675577156743,\n",
       " 'recall': 1.0,\n",
       " 'ground_truth': [{'file_path': 'contractnli/Grindrod%20SA%20Confidentiality%20and%20Non-Disclosure%20Undertaking.txt',\n",
       "   'span': [1942, 2219],\n",
       "   'answer': 'Confidential Information also excludes information in the public domain for a reason, other than a breach of this Confidentiality and Non-Disclosure Undertaking, with any party, or independently developed by the Vendor without reference to information provided by, Grindrod SA;'}],\n",
       " 'retrieved': [{'file_path': 'contractnli/Grindrod%20SA%20Confidentiality%20and%20Non-Disclosure%20Undertaking.txt',\n",
       "   'span': [0, 258],\n",
       "   'text': 'CONFIDENTIALITY AND NON-DISCLOSURE UNDERTAKING\\nConfidentiality and Non-Disclosure Undertaking Between Grindrod South Africa (Proprietary) Limited and _________________________________________________________ (Insert Full Name of Vendor Company or Individual)',\n",
       "   'score': 0.8957294975741699},\n",
       "  {'file_path': 'contractnli/tpi-non-disclosure-agreement_1.txt',\n",
       "   'span': [0, 24],\n",
       "   'text': 'NON-DISCLOSURE AGREEMENT',\n",
       "   'score': 0.8897352884852535},\n",
       "  {'file_path': 'contractnli/ExcelerateStandardNDAFormat.txt',\n",
       "   'span': [0, 24],\n",
       "   'text': 'NON-DISCLOSURE AGREEMENT',\n",
       "   'score': 0.8897352884852535},\n",
       "  {'file_path': 'contractnli/SupplementOne-NDA.txt',\n",
       "   'span': [0, 24],\n",
       "   'text': 'NON-DISCLOSURE AGREEMENT',\n",
       "   'score': 0.8897352779485413},\n",
       "  {'file_path': 'contractnli/QuickBooks-NDA-template.txt',\n",
       "   'span': [0, 24],\n",
       "   'text': 'Non-disclosure Agreement',\n",
       "   'score': 0.8897352779485413},\n",
       "  {'file_path': 'contractnli/lti-two-way-cda-template.txt',\n",
       "   'span': [0, 24],\n",
       "   'text': 'NON-DISCLOSURE AGREEMENT',\n",
       "   'score': 0.8897352779485413},\n",
       "  {'file_path': 'contractnli/Grindrod%20SA%20Confidentiality%20and%20Non-Disclosure%20Undertaking.txt',\n",
       "   'span': [1490, 1940],\n",
       "   'text': '. Confidential Information shall also include any other information that is marked as \"Confidential\" or should reasonably be considered confidential Confidential Information excludes information which is already in the possession, or under the control of the Vendor otherwise than as a result of having been disclosed by Grindrod SA to the Vendor or as a result of the preparation and execution of a proposal or any other interaction with Grindrod SA',\n",
       "   'score': 0.8715931037867543},\n",
       "  {'file_path': 'contractnli/Grindrod%20SA%20Confidentiality%20and%20Non-Disclosure%20Undertaking.txt',\n",
       "   'span': [1940, 2219],\n",
       "   'text': '. Confidential Information also excludes information in the public domain for a reason, other than a breach of this Confidentiality and Non-Disclosure Undertaking, with any party, or independently developed by the Vendor without reference to information provided by, Grindrod SA;',\n",
       "   'score': 0.8710071466886194},\n",
       "  {'file_path': 'contractnli/Grindrod%20SA%20Confidentiality%20and%20Non-Disclosure%20Undertaking.txt',\n",
       "   'span': [4551, 5046],\n",
       "   'text': '\\n 3.5. to bring to the attention of Grindrod SA, in writing, any abuse or unauthorized disclosure of such Confidential Information of which the Vendor becomes aware,\\n 3.6 that the provisions of this Confidentiality and Non-Disclosure Undertaking shall survive the termination or expiration of any Agreement / understanding / Request for Quotation / Contract / Purchase Order or any interaction, with Grindrod SA, of whatsoever nature.\\nSigned at this day of 2016\\n(For and on behalf of the Vendor)',\n",
       "   'score': 0.8673836556815924},\n",
       "  {'file_path': 'contractnli/simply-fashion---standard-nda.txt',\n",
       "   'span': [0, 44],\n",
       "   'text': 'CONFIDENTIALITY AND NON-DISCLOSURE AGREEMENT',\n",
       "   'score': 0.8670976471675974}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qidx = random_number = random.randint(1, len(results))\n",
    "results[qidx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
