{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset    \n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import logging\n",
    "import transformers\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "bert_model = AutoModelForPreTraining.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all file as DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './data/json_output/contractnli/llama3/'\n",
    "\n",
    "dfs_dict = {}\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.json') and file_name.startswith('query_answer'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            dfs_dict[file_name] = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in dfs_dict:\n",
      "query_answer_baseline_contractnli_llama3_k1.json\n",
      "query_answer_baseline_contractnli_llama3_k10.json\n",
      "query_answer_baseline_contractnli_llama3_k3.json\n",
      "query_answer_baseline_contractnli_llama3_k5.json\n",
      "query_answer_manually_written_contractnli_llama3_k1.json\n",
      "query_answer_manually_written_contractnli_llama3_k10.json\n",
      "query_answer_manually_written_contractnli_llama3_k3.json\n",
      "query_answer_manually_written_contractnli_llama3_k5.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Files in dfs_dict:\")\n",
    "for file_name in dfs_dict.keys():\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate using RAGAS + BERT F1 + Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_file(file_index, num_queries = -1):\n",
    "    file_name = list(dfs_dict.keys())[file_index]\n",
    "\n",
    "    if num_queries == -1:\n",
    "        num_queries = dfs_dict[file_name].shape[0]\n",
    "\n",
    "    cur_df = dfs_dict[file_name].head(num_queries)  # Process only the specified number of queries\n",
    "    print(f\"Evaluating file: {file_name}\")\n",
    "\n",
    "    # # RAGAS evaluation\n",
    "    # evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5))\n",
    "    # dataset_ragas = Dataset.from_pandas(cur_df) \n",
    "    # # display(cur_df)   \n",
    "    # ragas_result = evaluate(\n",
    "    #     dataset_ragas,\n",
    "    #     metrics=[\n",
    "    #         faithfulness,\n",
    "    #         answer_relevancy\n",
    "    #     ],\n",
    "    #     llm=evaluator_llm\n",
    "    # )\n",
    "\n",
    "    # ragas_result_dict = ragas_result.__repr__()     \n",
    "    # ragas_result_dict = {\"faithfulness\": 0, \"answer_relevancy\": 0}   \n",
    "\n",
    "    # BERT and ROUGE evaluation\n",
    "    responses = cur_df['response'].tolist()\n",
    "    contexts = cur_df['retrieved_contexts'].tolist()\n",
    "\n",
    "    # Initialize scorers\n",
    "    # bert_scorer_obj = BERTScorer(lang=\"en\", model_type=\"nlpaueb/legal-bert-base-uncased\", num_layers=12)\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "    bert_f1_scores = []\n",
    "    rouge1_recall_scores = []\n",
    "    rouge2_recall_scores = []\n",
    "    rougeL_recall_scores = []\n",
    "\n",
    "    for response, context_list in zip(responses, contexts):\n",
    "        # Join all contexts into single reference\n",
    "        if isinstance(context_list, str):\n",
    "            single_ref = context_list\n",
    "        else:\n",
    "            single_ref = \" \".join(context_list)\n",
    "\n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = rouge_scorer_obj.score(single_ref, response)\n",
    "        rouge1_recall_scores.append(rouge_scores[\"rouge1\"].recall)\n",
    "        rouge2_recall_scores.append(rouge_scores[\"rouge2\"].recall)\n",
    "        rougeL_recall_scores.append(rouge_scores[\"rougeL\"].recall)\n",
    "\n",
    "    # Calculate average scores\n",
    "    avg_rouge1_recall = np.mean(rouge1_recall_scores)\n",
    "    avg_rouge2_recall = np.mean(rouge2_recall_scores)\n",
    "    avg_rougeL_recall = np.mean(rougeL_recall_scores)\n",
    "\n",
    "    results = {\n",
    "        # 'ragas_metrics': ragas_result_dict,\n",
    "        'rouge1_recall': avg_rouge1_recall,\n",
    "        'rouge2_recall': avg_rouge2_recall,\n",
    "        'rougeL_recall': avg_rougeL_recall\n",
    "    }\n",
    "\n",
    "    results_path = os.path.join(folder_path, \"evaluation_results.json\")\n",
    "\n",
    "    # Load existing results if the file exists\n",
    "    if os.path.exists(results_path):\n",
    "        with open(results_path, \"r\") as results_file:\n",
    "            all_results = json.load(results_file)\n",
    "    else:\n",
    "        all_results = {}\n",
    "\n",
    "    # Add or update metrics for the current file\n",
    "    if file_name not in all_results:\n",
    "        all_results[file_name] = {}\n",
    "    all_results[file_name].update(results)\n",
    "\n",
    "    # Save the updated results back to the file\n",
    "    with open(results_path, \"w\") as results_file:\n",
    "        json.dump(all_results, results_file, indent=4)\n",
    "\n",
    "    print(f\"Results for {file_name}:\")\n",
    "    print(f\"ROUGE-1 Recall: {avg_rouge1_recall:.3f}\")\n",
    "    print(f\"ROUGE-2 Recall: {avg_rouge2_recall:.3f}\") \n",
    "    print(f\"ROUGE-L Recall: {avg_rougeL_recall:.3f}\")\n",
    "    print(f\"Results saved to {results_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating file: query_answer_baseline_contractnli_llama3_k1.json\n",
      "Results for query_answer_baseline_contractnli_llama3_k1.json:\n",
      "ROUGE-1 Recall: 0.438\n",
      "ROUGE-2 Recall: 0.236\n",
      "ROUGE-L Recall: 0.335\n",
      "Results saved to ./data/json_output/contractnli/llama3/evaluation_results.json\n",
      "\n",
      "Evaluating file: query_answer_baseline_contractnli_llama3_k10.json\n",
      "Results for query_answer_baseline_contractnli_llama3_k10.json:\n",
      "ROUGE-1 Recall: 0.145\n",
      "ROUGE-2 Recall: 0.061\n",
      "ROUGE-L Recall: 0.092\n",
      "Results saved to ./data/json_output/contractnli/llama3/evaluation_results.json\n",
      "\n",
      "Evaluating file: query_answer_baseline_contractnli_llama3_k3.json\n",
      "Results for query_answer_baseline_contractnli_llama3_k3.json:\n",
      "ROUGE-1 Recall: 0.226\n",
      "ROUGE-2 Recall: 0.114\n",
      "ROUGE-L Recall: 0.160\n",
      "Results saved to ./data/json_output/contractnli/llama3/evaluation_results.json\n",
      "\n",
      "Evaluating file: query_answer_baseline_contractnli_llama3_k5.json\n",
      "Results for query_answer_baseline_contractnli_llama3_k5.json:\n",
      "ROUGE-1 Recall: 0.221\n",
      "ROUGE-2 Recall: 0.095\n",
      "ROUGE-L Recall: 0.146\n",
      "Results saved to ./data/json_output/contractnli/llama3/evaluation_results.json\n",
      "\n",
      "Evaluating file: query_answer_manually_written_contractnli_llama3_k1.json\n",
      "Results for query_answer_manually_written_contractnli_llama3_k1.json:\n",
      "ROUGE-1 Recall: 0.800\n",
      "ROUGE-2 Recall: 0.635\n",
      "ROUGE-L Recall: 0.705\n",
      "Results saved to ./data/json_output/contractnli/llama3/evaluation_results.json\n",
      "\n",
      "Evaluating file: query_answer_manually_written_contractnli_llama3_k10.json\n",
      "Results for query_answer_manually_written_contractnli_llama3_k10.json:\n",
      "ROUGE-1 Recall: 0.200\n",
      "ROUGE-2 Recall: 0.088\n",
      "ROUGE-L Recall: 0.114\n",
      "Results saved to ./data/json_output/contractnli/llama3/evaluation_results.json\n",
      "\n",
      "Evaluating file: query_answer_manually_written_contractnli_llama3_k3.json\n",
      "Results for query_answer_manually_written_contractnli_llama3_k3.json:\n",
      "ROUGE-1 Recall: 0.435\n",
      "ROUGE-2 Recall: 0.206\n",
      "ROUGE-L Recall: 0.274\n",
      "Results saved to ./data/json_output/contractnli/llama3/evaluation_results.json\n",
      "\n",
      "Evaluating file: query_answer_manually_written_contractnli_llama3_k5.json\n",
      "Results for query_answer_manually_written_contractnli_llama3_k5.json:\n",
      "ROUGE-1 Recall: 0.339\n",
      "ROUGE-2 Recall: 0.149\n",
      "ROUGE-L Recall: 0.194\n",
      "Results saved to ./data/json_output/contractnli/llama3/evaluation_results.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_file_count = len(dfs_dict)  # Calculate total_file_count from dfs_dict\n",
    "for file_index in range(0, total_file_count):\n",
    "    evaluate_file(file_index=file_index, num_queries=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from prince\n",
    "\n",
    "def compute_rouge_recall(retrieved_chunks, model_responses):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    recall_scores = []\n",
    "\n",
    "    for ref_chunks, pred in zip(retrieved_chunks, model_responses):\n",
    "        ref_text = \" \".join(ref_chunks)\n",
    "        scores = scorer.score(ref_text, pred)\n",
    "\n",
    "        # Compute the average recall score instead of using just ROUGE-1\n",
    "        avg_recall = (scores[\"rouge1\"].recall + scores[\"rouge2\"].recall + scores[\"rougeL\"].recall) / 3\n",
    "        recall_scores.append(avg_recall)\n",
    "\n",
    "    return recall_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def reorder_evaluation_results(file_path):\n",
    "    # Read the JSON file\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Create a sorted dictionary to store reorganized results\n",
    "    organized_results = {\n",
    "        'baseline': {},\n",
    "        'CoT': {},\n",
    "        'manually_written': {}\n",
    "    }\n",
    "    \n",
    "    # Process each file result\n",
    "    for filename, metrics in data.items():\n",
    "        # Extract k value from filename\n",
    "        if '_k' in filename:\n",
    "            k_value = int(filename.split('_k')[-1].split('.')[0])\n",
    "            k_key = f'k{k_value}'\n",
    "            \n",
    "            # Determine the type and store in organized format\n",
    "            if 'baseline' in filename:\n",
    "                organized_results['baseline'][k_key] = metrics\n",
    "            elif 'CoT' in filename:\n",
    "                organized_results['CoT'][k_key] = metrics\n",
    "            elif 'manually_written' in filename:\n",
    "                organized_results['manually_written'][k_key] = metrics\n",
    "    \n",
    "    # Sort k-values within each type\n",
    "    for response_type in organized_results:\n",
    "        organized_results[response_type] = dict(sorted(\n",
    "            organized_results[response_type].items(),\n",
    "            key=lambda x: int(x[0][1:])  # Sort by k value\n",
    "        ))\n",
    "    \n",
    "    # Write the reorganized results\n",
    "    output_path = file_path.replace('.json', '_reorganized_by_type.json')\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(organized_results, f, indent=4)\n",
    "    \n",
    "    return organized_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline': {'k1': {'ragas_metrics': \"{'faithfulness': 0.6510, 'answer_relevancy': 0.3157}\",\n",
       "   'bert_f1': 0.6471516858671129,\n",
       "   'rouge_recall': 0.1655380309211203,\n",
       "   'rouge1_recall': 0.23262855923782638,\n",
       "   'rouge2_recall': 0.09323326788258988,\n",
       "   'rougeL_recall': 0.1707522656429445},\n",
       "  'k3': {'ragas_metrics': \"{'faithfulness': 0.8311, 'answer_relevancy': 0.6195}\",\n",
       "   'bert_f1': 0.7001845624643502,\n",
       "   'rouge_recall': 0.127273956099688,\n",
       "   'rouge1_recall': 0.18232571670484188,\n",
       "   'rouge2_recall': 0.07312573913446306,\n",
       "   'rougeL_recall': 0.12637041245975889},\n",
       "  'k5': {'ragas_metrics': \"{'faithfulness': 0.8299, 'answer_relevancy': 0.6980}\",\n",
       "   'bert_f1': 0.7004778944339949,\n",
       "   'rouge_recall': 0.09338551698081571,\n",
       "   'rouge1_recall': 0.131375877522047,\n",
       "   'rouge2_recall': 0.055975005847801296,\n",
       "   'rougeL_recall': 0.09280566757259875},\n",
       "  'k10': {'ragas_metrics': \"{'faithfulness': 0.8438, 'answer_relevancy': 0.7899}\",\n",
       "   'bert_f1': 0.7053153364314246,\n",
       "   'rouge_recall': 0.059093593525550214,\n",
       "   'rouge1_recall': 0.08093383074885364,\n",
       "   'rouge2_recall': 0.038017355373234064,\n",
       "   'rougeL_recall': 0.05832959445456292}},\n",
       " 'CoT': {'k1': {'ragas_metrics': \"{'faithfulness': 0.6053, 'answer_relevancy': 0.3772}\",\n",
       "   'bert_f1': 0.6667347707084774,\n",
       "   'rouge_recall': 0.15557610774215946,\n",
       "   'rouge1_recall': 0.23193293846576657,\n",
       "   'rouge2_recall': 0.07001240084324595,\n",
       "   'rougeL_recall': 0.16478298391746596},\n",
       "  'k3': {'ragas_metrics': \"{'faithfulness': 0.6494, 'answer_relevancy': 0.6954}\",\n",
       "   'bert_f1': 0.6818613262520623,\n",
       "   'rouge_recall': 0.08758396013349685,\n",
       "   'rouge1_recall': 0.13181968996875335,\n",
       "   'rouge2_recall': 0.041509831564807906,\n",
       "   'rougeL_recall': 0.08942235886692951},\n",
       "  'k5': {'ragas_metrics': \"{'faithfulness': 0.6916, 'answer_relevancy': 0.7648}\",\n",
       "   'bert_f1': 0.6793125714223409,\n",
       "   'rouge_recall': 0.06031036442861096,\n",
       "   'rouge1_recall': 0.08885997585881121,\n",
       "   'rouge2_recall': 0.030265892768314172,\n",
       "   'rougeL_recall': 0.061805224658707525},\n",
       "  'k10': {'ragas_metrics': \"{'faithfulness': 0.7419, 'answer_relevancy': 0.8431}\",\n",
       "   'bert_f1': 0.6818058128209458,\n",
       "   'rouge_recall': 0.03735362418257778,\n",
       "   'rouge1_recall': 0.0529783954437238,\n",
       "   'rouge2_recall': 0.020701692223740208,\n",
       "   'rougeL_recall': 0.038380784880269324}},\n",
       " 'manually_written': {'k1': {'ragas_metrics': \"{'faithfulness': 0.7896, 'answer_relevancy': 0.4551}\",\n",
       "   'bert_f1': 0.745835859750964,\n",
       "   'rouge_recall': 0.47657316697616253,\n",
       "   'rouge1_recall': 0.5879379116722332,\n",
       "   'rouge2_recall': 0.36738295447041436,\n",
       "   'rougeL_recall': 0.4743986347858402},\n",
       "  'k3': {'ragas_metrics': \"{'faithfulness': 0.8610, 'answer_relevancy': 0.5993}\",\n",
       "   'bert_f1': 0.7336989241777007,\n",
       "   'rouge_recall': 0.257389178568886,\n",
       "   'rouge1_recall': 0.3394555351631309,\n",
       "   'rouge2_recall': 0.186976969191896,\n",
       "   'rougeL_recall': 0.24573503135163138},\n",
       "  'k5': {'ragas_metrics': \"{'faithfulness': 0.8879, 'answer_relevancy': 0.6112}\",\n",
       "   'bert_f1': 0.7326568076291036,\n",
       "   'rouge_recall': 0.19457910253543267,\n",
       "   'rouge1_recall': 0.25615324171872855,\n",
       "   'rouge2_recall': 0.14279364300266606,\n",
       "   'rougeL_recall': 0.1847904228849036},\n",
       "  'k10': {'ragas_metrics': \"{'faithfulness': 0.9047, 'answer_relevancy': 0.7325}\",\n",
       "   'bert_f1': 0.7275234669754186,\n",
       "   'rouge_recall': 0.1124899037681614,\n",
       "   'rouge1_recall': 0.1449609015164644,\n",
       "   'rouge2_recall': 0.08401022440799095,\n",
       "   'rougeL_recall': 0.10849858538002867}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file_path_contractnli = \"data/json_output/contractnli/gpt/evaluation_results.json\"\n",
    "# # file_path_cuad = \"data/json_output/cuad/llama3/evaluation_results.json\"\n",
    "# file_path_maud = \"data/json_output/maud/llama3/evaluation_results.json\"\n",
    "# file_path_privacy_qa = \"data/json_output/privacyqa/llama3/evaluation_results.json\"\n",
    "\n",
    "\n",
    "file_path_contractnli = \"data/json_output/ZZZ_llama_cot/contractnli/gpt/evaluation_results.json\"\n",
    "\n",
    "\n",
    "reorder_evaluation_results(file_path_contractnli)\n",
    "# reorder_evaluation_results(file_path_cuad)\n",
    "# reorder_evaluation_results(file_path_maud)\n",
    "# reorder_evaluation_results(file_path_privacy_qa)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "response_gen_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
