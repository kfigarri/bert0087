{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset    \n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.embeddings.base import (\n",
    "    BaseRagasEmbeddings,\n",
    "    LangchainEmbeddingsWrapper,\n",
    "    embedding_factory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_queries = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './answer_relevancy_qualitative_2/'\n",
    "\n",
    "dfs_dict = {}\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.json') and file_name.startswith('baseline'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            dfs_dict[file_name] = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in dfs_dict:\n",
      "baseline_gpt4omini_k1.json\n",
      "baseline_gpt4omini_k10.json\n",
      "baseline_gpt4omini_k3.json\n",
      "baseline_gpt4omini_k5.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Files in dfs_dict:\")\n",
    "for file_name in dfs_dict.keys():\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating file: baseline_gpt4omini_k1.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_name = list(dfs_dict.keys())[0]\n",
    "cur_df = dfs_dict[file_name].head(num_queries)\n",
    "print(f\"Evaluating file: {file_name}\")\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5))\n",
    "# dataset_ragas = Dataset.from_pandas(cur_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>response</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Consider the Non-Disclosure Agreement between ...</td>\n",
       "      <td>The context provided does not explicitly state...</td>\n",
       "      <td>[4 Definition of Confidential Information\\n“Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Consider the Non-Disclosure Agreement between ...</td>\n",
       "      <td>No, the document does not state that Confident...</td>\n",
       "      <td>[4 Definition of Confidential Information\\n“Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consider the Non-Disclosure Agreement between ...</td>\n",
       "      <td>Yes, the document mentions that obligations re...</td>\n",
       "      <td>[9 Term\\nThis Agreement shall be effective as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Consider the Non-Disclosure Agreement between ...</td>\n",
       "      <td>The provided context does not specify whether ...</td>\n",
       "      <td>[Mentor shall notify Organiser immediately in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Consider the Non-Disclosure Agreement between ...</td>\n",
       "      <td>No, the Non-Disclosure Agreement does not allo...</td>\n",
       "      <td>[Mentor shall not disclose any Confidential In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Consider the Non-Disclosure Agreement between ...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>[Mentor shall not disclose any Confidential In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Consider the Non-Disclosure Agreement between ...</td>\n",
       "      <td>The context provided does not explicitly addre...</td>\n",
       "      <td>[4 Definition of Confidential Information\\n“Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Consider the Non-Disclosure Agreement between ...</td>\n",
       "      <td>No, the document does not allow the Receiving ...</td>\n",
       "      <td>[Mentor shall not disclose any Confidential In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Consider the Non-Disclosure Agreement between ...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>[NON-DISCLOSURE AGREEMENT AND TERMS OF PARTICI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Consider DBT's Mutual Non-Disclosure Agreement...</td>\n",
       "      <td>Yes, the document indicates that the Agreement...</td>\n",
       "      <td>[5. No Further Rights All Confidential Informa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Consider the Non-Disclosure Agreement between ...   \n",
       "1  Consider the Non-Disclosure Agreement between ...   \n",
       "2  Consider the Non-Disclosure Agreement between ...   \n",
       "3  Consider the Non-Disclosure Agreement between ...   \n",
       "4  Consider the Non-Disclosure Agreement between ...   \n",
       "5  Consider the Non-Disclosure Agreement between ...   \n",
       "6  Consider the Non-Disclosure Agreement between ...   \n",
       "7  Consider the Non-Disclosure Agreement between ...   \n",
       "8  Consider the Non-Disclosure Agreement between ...   \n",
       "9  Consider DBT's Mutual Non-Disclosure Agreement...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The context provided does not explicitly state...   \n",
       "1  No, the document does not state that Confident...   \n",
       "2  Yes, the document mentions that obligations re...   \n",
       "3  The provided context does not specify whether ...   \n",
       "4  No, the Non-Disclosure Agreement does not allo...   \n",
       "5                                      I don't know.   \n",
       "6  The context provided does not explicitly addre...   \n",
       "7  No, the document does not allow the Receiving ...   \n",
       "8                                      I don't know.   \n",
       "9  Yes, the document indicates that the Agreement...   \n",
       "\n",
       "                                  retrieved_contexts  \n",
       "0  [4 Definition of Confidential Information\\n“Co...  \n",
       "1  [4 Definition of Confidential Information\\n“Co...  \n",
       "2  [9 Term\\nThis Agreement shall be effective as ...  \n",
       "3  [Mentor shall notify Organiser immediately in ...  \n",
       "4  [Mentor shall not disclose any Confidential In...  \n",
       "5  [Mentor shall not disclose any Confidential In...  \n",
       "6  [4 Definition of Confidential Information\\n“Co...  \n",
       "7  [Mentor shall not disclose any Confidential In...  \n",
       "8  [NON-DISCLOSURE AGREEMENT AND TERMS OF PARTICI...  \n",
       "9  [5. No Further Rights All Confidential Informa...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(cur_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import SingleTurnSample \n",
    "from ragas.metrics import ResponseRelevancy\n",
    "\n",
    "samples = []\n",
    "for _, row in cur_df.iterrows():\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=row['user_input'],\n",
    "        response=row['response'],\n",
    "        retrieved_contexts=row['retrieved_contexts']\n",
    "    )\n",
    "    samples.append(sample)\n",
    "\n",
    "# evaluator_embeddings = \n",
    "\n",
    "response_relevancy_scorer = ResponseRelevancy(llm=evaluator_llm, embeddings=embedding_factory())\n",
    "\n",
    "for sample in samples:\n",
    "    with open('output.txt', 'a') as f:        \n",
    "        print(\"{\", file=f)\n",
    "    with open('output.txt', 'a') as f:        \n",
    "        print(f\"\\\"user_input\\\" : \\\"{sample.user_input}\\\",\", file=f)\n",
    "    \n",
    "    score = await response_relevancy_scorer.single_turn_ascore(sample)    \n",
    "    # with open('output.txt', 'a') as f:        \n",
    "        # print(score, file=f)\n",
    "\n",
    "    with open('output.txt', 'a') as f:        \n",
    "        print(\"},\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_file(num_queries: int, file_index: int, output_file_name: str = None):\n",
    "    \"\"\"\n",
    "    Evaluate a single file using ResponseRelevancy metric\n",
    "    \n",
    "    Args:\n",
    "        num_queries: Number of queries to evaluate\n",
    "        file_index: Index of the file to evaluate in dfs_dict\n",
    "        output_file_name: Optional name of output file. If None, uses 'output.txt'\n",
    "    \"\"\"\n",
    "    evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5))\n",
    "    response_relevancy_scorer = ResponseRelevancy(llm=evaluator_llm, embeddings=embedding_factory())\n",
    "    \n",
    "    # Get the specified file\n",
    "    file_name = list(dfs_dict.keys())[file_index]\n",
    "    print(f\"\\nEvaluating file: {file_name}\")\n",
    "    \n",
    "    # Set output file name\n",
    "    output_file = output_file_name if output_file_name is not None else 'output.txt'\n",
    "    \n",
    "    # Get data and display\n",
    "    cur_df = dfs_dict[file_name].head(num_queries)\n",
    "    # display(cur_df)\n",
    "    \n",
    "    # Create samples\n",
    "    samples = []\n",
    "    for _, row in cur_df.iterrows():\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=row['user_input'],\n",
    "            response=row['response'],\n",
    "            retrieved_contexts=row['retrieved_contexts']\n",
    "        )\n",
    "        samples.append(sample)\n",
    "    \n",
    "    # Evaluate each sample\n",
    "    for sample in samples:\n",
    "        with open(output_file, 'a') as f:        \n",
    "            print(\"{\", file=f)\n",
    "        with open(output_file, 'a') as f:        \n",
    "            print(f\"\\\"user_input\\\" : \\\"{sample.user_input}\\\",\", file=f)\n",
    "        \n",
    "        score = await response_relevancy_scorer.single_turn_ascore(sample)\n",
    "        \n",
    "        with open(output_file, 'a') as f:        \n",
    "            print(\"},\", file=f)\n",
    "\n",
    "# Example usage:\n",
    "# await evaluate_file(num_queries=10, file_index=0)  # Evaluate first file\n",
    "# await evaluate_file(num_queries=10, file_index=1, output_file_name=\"custom_output.txt\")  # Evaluate second file with custom output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating file: baseline_gpt4omini_k5.json\n"
     ]
    }
   ],
   "source": [
    "await evaluate_file(num_queries=194, file_index=3, output_file_name=\"output.txt\")  # Evaluate second file with custom output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "response_gen_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
