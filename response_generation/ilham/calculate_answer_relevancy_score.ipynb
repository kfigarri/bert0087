{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine_sim_mean: 0.9073\n",
      "Average final_score: 0.4458\n",
      "Number of final_score of 0: 98\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSON file\n",
    "with open('answer_relevancy_qualitative_output/verbose_AR_prompttune_gpt4omini_k1.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Calculate averages\n",
    "cosine_sim_mean_avg = sum(item['cosine_sim_mean'] for item in data) / len(data)\n",
    "final_score_avg = sum(item['final_score'] for item in data) / len(data)\n",
    "\n",
    "# Count number of final_score of 0\n",
    "zero_score_count = sum(1 for item in data if item['final_score'] == 0)\n",
    "\n",
    "print(f\"Average cosine_sim_mean: {cosine_sim_mean_avg:.4f}\")\n",
    "print(f\"Average final_score: {final_score_avg:.4f}\")\n",
    "print(f\"Number of final_score of 0: {zero_score_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine_sim_mean: 0.9055\n",
      "Average final_score: 0.5992\n",
      "Number of final_score of 0: 65\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSON file\n",
    "with open('answer_relevancy_qualitative_output/verbose_AR_prompttune_gpt4omini_k3.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Calculate averages\n",
    "cosine_sim_mean_avg = sum(item['cosine_sim_mean'] for item in data) / len(data)\n",
    "final_score_avg = sum(item['final_score'] for item in data) / len(data)\n",
    "\n",
    "# Count number of final_score of 0\n",
    "zero_score_count = sum(1 for item in data if item['final_score'] == 0)\n",
    "\n",
    "print(f\"Average cosine_sim_mean: {cosine_sim_mean_avg:.4f}\")\n",
    "print(f\"Average final_score: {final_score_avg:.4f}\")\n",
    "print(f\"Number of final_score of 0: {zero_score_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine_sim_mean: 0.9068\n",
      "Average final_score: 0.6211\n",
      "Number of final_score of 0: 61\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSON file\n",
    "with open('answer_relevancy_qualitative_output/verbose_AR_prompttune_gpt4omini_k5.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Calculate averages\n",
    "cosine_sim_mean_avg = sum(item['cosine_sim_mean'] for item in data) / len(data)\n",
    "final_score_avg = sum(item['final_score'] for item in data) / len(data)\n",
    "\n",
    "# Count number of final_score of 0\n",
    "zero_score_count = sum(1 for item in data if item['final_score'] == 0)\n",
    "\n",
    "print(f\"Average cosine_sim_mean: {cosine_sim_mean_avg:.4f}\")\n",
    "print(f\"Average final_score: {final_score_avg:.4f}\")\n",
    "print(f\"Number of final_score of 0: {zero_score_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine_sim_mean: 0.9054\n",
      "Average final_score: 0.7375\n",
      "Number of final_score of 0: 36\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSON file\n",
    "with open('answer_relevancy_qualitative_output/verbose_AR_prompttune_gpt4omini_k10.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Calculate averages\n",
    "cosine_sim_mean_avg = sum(item['cosine_sim_mean'] for item in data) / len(data)\n",
    "final_score_avg = sum(item['final_score'] for item in data) / len(data)\n",
    "\n",
    "# Count number of final_score of 0\n",
    "zero_score_count = sum(1 for item in data if item['final_score'] == 0)\n",
    "\n",
    "print(f\"Average cosine_sim_mean: {cosine_sim_mean_avg:.4f}\")\n",
    "print(f\"Average final_score: {final_score_avg:.4f}\")\n",
    "print(f\"Number of final_score of 0: {zero_score_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for verbose_AR_baseline_gpt4omini_k1.json:\n",
      "Average cosine_sim_mean: 0.8746\n",
      "Average final_score: 0.3112\n",
      "Number of final_score of 0: 128\n",
      "\n",
      "Results for verbose_AR_baseline_gpt4omini_k10.json:\n",
      "Average cosine_sim_mean: 0.9141\n",
      "Average final_score: 0.7857\n",
      "Number of final_score of 0: 27\n",
      "\n",
      "Results for verbose_AR_baseline_gpt4omini_k3.json:\n",
      "Average cosine_sim_mean: 0.9145\n",
      "Average final_score: 0.6199\n",
      "Number of final_score of 0: 62\n",
      "\n",
      "Results for verbose_AR_baseline_gpt4omini_k5.json:\n",
      "Average cosine_sim_mean: 0.9181\n",
      "Average final_score: 0.6935\n",
      "Number of final_score of 0: 47\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize results list\n",
    "results = []\n",
    "\n",
    "# Get all JSON files in the directory\n",
    "folder_path = 'answer_relevancy_qualitative_output_2'\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "# Process each file\n",
    "for json_file in json_files:\n",
    "    file_path = os.path.join(folder_path, json_file)\n",
    "    \n",
    "    # Read the JSON file\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cosine_sim_mean_avg = sum(item['cosine_sim_mean'] for item in data) / len(data)\n",
    "    final_score_avg = sum(item['final_score'] for item in data) / len(data)\n",
    "    zero_score_count = sum(1 for item in data if item['final_score'] == 0)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'file_name': json_file,\n",
    "        'cosine_sim_mean_avg': cosine_sim_mean_avg,\n",
    "        'final_score_avg': final_score_avg,\n",
    "        'zero_score_count': zero_score_count\n",
    "    })\n",
    "    \n",
    "    # Print results for each file\n",
    "    print(f\"\\nResults for {json_file}:\")\n",
    "    print(f\"Average cosine_sim_mean: {cosine_sim_mean_avg:.4f}\")\n",
    "    print(f\"Average final_score: {final_score_avg:.4f}\")\n",
    "    print(f\"Number of final_score of 0: {zero_score_count}\")\n",
    "\n",
    "# Save all results to a single JSON file\n",
    "with open('answer_relevancy_verbose_eval.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "response_gen_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
