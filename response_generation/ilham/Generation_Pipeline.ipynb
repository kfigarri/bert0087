{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "from pydantic import BaseModel, computed_field\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from langchain import hub\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import model dependencies and load API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for OPENAI\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "api_key_openai = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for LLAMA (via GROQ)\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "api_key_groq = \"YOUR API KEY HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from info retrieval (will have maximum K of context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_file = \"./data/json_output/contractnli.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(current_file) as f:\n",
    "    QnA_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get query and K number of context\n",
    "def get_query_from_json_at_K(index=0, k_context=3):\n",
    "    \n",
    "    qna_data_query = QnA_data[index][\"query\"]\n",
    "    retrieved_chunks = QnA_data[index][\"retrieved_chunks_unranked\"]\n",
    "\n",
    "    contexts_from_json = []\n",
    "\n",
    "    for i in range(min(k_context, len(retrieved_chunks))):\n",
    "        context = retrieved_chunks[i]\n",
    "        cur_context = {}\n",
    "        cur_context[\"file_path\"] = context[\"filepath\"]\n",
    "        cur_context[\"span\"] = context[\"span\"]\n",
    "        cur_context[\"chunk\"] = context[\"text\"]\n",
    "\n",
    "        contexts_from_json.append(cur_context)\n",
    "    \n",
    "    return qna_data_query, contexts_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Consider \"Fiverr\"\\'s privacy policy; who can see which tasks i hire workers for?',\n",
       " [{'file_path': 'privacy_qa/Fiverr.txt',\n",
       "   'span': [1011, 1388],\n",
       "   'chunk': 'Information that you choose to publish on the Site (photos, videos, text, music, reviews, deliveries) - is no longer private, just like any information you publish online.\\n  Technical information that is gathered by our systems, or third party systems, automatically may be used for Site operation, optimization, analytics, content promotion and enhancement of user experience.'},\n",
       "  {'file_path': 'privacy_qa/Fiverr.txt',\n",
       "   'span': [173, 527],\n",
       "   'chunk': 'We do not disclose it to others except as disclosed in this Policy or required to provide you with the services of the Site and mobile applications, meaning - to allow you to buy, sell, share the information you want to share on the Site; to contribute on the forum; pay for products; post reviews and so on; or where we have a legal obligation to do so.'},\n",
       "  {'file_path': 'privacy_qa/Fiverr.txt',\n",
       "   'span': [12571, 12941],\n",
       "   'chunk': \"We are not exposed to the payment information provided to our payment vendors, and this information is subject to the privacy policy applicable to the payment vendor; and\\n   your personal information may be disclosed if we go through a business transition such as a merger, sale, transfer of all or a portion of Fiverr's assets, acquisition, bankruptcy or similar event.\"}])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example use\n",
    "get_query_from_json_at_K(0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response generator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseGenerator:\n",
    "    class State(TypedDict):\n",
    "        question : str\n",
    "        context : List[Document]\n",
    "        answer: str\n",
    "\n",
    "    def __init__(self, prompt : PromptTemplate, llm : BaseChatModel):\n",
    "        self.llm = llm\n",
    "        self.prompt = prompt\n",
    "\n",
    "        graph_builder = StateGraph(self.State)\n",
    "        graph_builder.add_sequence([self.generate])\n",
    "        graph_builder.add_edge(START, \"generate\")\n",
    "        self.graph = graph_builder.compile()\n",
    "\n",
    "    def generate(self, state : State):\n",
    "        context_doc_message = \"\\n\\n\".join(doc for doc in state[\"context\"])\n",
    "        message = self.prompt.invoke({\"question\":state[\"question\"], \"context\":context_doc_message})\n",
    "        response = self.llm.invoke(message)\n",
    "\n",
    "        return({\"answer\":response})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate response functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  \n",
    "\n",
    "def generate_response_with_context_at_K(response_generator: ResponseGenerator, size=10, k_context=3, JSON_CoT=False):\n",
    "    qna_context_list = []\n",
    "\n",
    "    # Wrap the range iterator with tqdm for progress tracking\n",
    "    for i in tqdm(range(0, size), desc=\"Generating responses\"):\n",
    "        query, contexts = get_query_from_json_at_K(index=i, k_context=k_context)            \n",
    "        output = response_generator.graph.invoke({\"question\": query, \"context\": [context[\"chunk\"] for context in contexts]})\n",
    "        \n",
    "        user_input = query\n",
    "        retrieved_contexts = [context[\"chunk\"] for context in contexts]\n",
    "\n",
    "        if JSON_CoT:\n",
    "            # Prepare the raw response for later parsing\n",
    "            raw_response = output[\"answer\"].content.strip()\n",
    "            qna_context_list.append([user_input, raw_response, retrieved_contexts])\n",
    "\n",
    "            dataset_df = pd.DataFrame(qna_context_list, columns=[\"user_input\", \"raw_response\", \"retrieved_contexts\"])\n",
    "\n",
    "        else:\n",
    "            response = output[\"answer\"].content\n",
    "            qna_context_list.append([user_input, response, retrieved_contexts])\n",
    "\n",
    "            dataset_df = pd.DataFrame(qna_context_list, columns=[\"user_input\", \"response\", \"retrieved_contexts\"])\n",
    "    \n",
    "    return dataset_df\n",
    "def generate_response_with_context_at_K(response_generator: ResponseGenerator, size = 10, k_context=3, JSON_CoT=False):\n",
    "    qna_context_list = []\n",
    "\n",
    "    # use tqdm here! \n",
    "    for i in tqdm(range(0, size), desc=\"Generating responses\"):\n",
    "        query, contexts = get_query_from_json_at_K(index=i, k_context = k_context)            \n",
    "        output = response_generator.graph.invoke({\"question\": query, \"context\": [context[\"chunk\"] for context in contexts]})\n",
    "        \n",
    "        user_input = query\n",
    "        retrieved_contexts = [context[\"chunk\"] for context in contexts]\n",
    "\n",
    "        if JSON_CoT:\n",
    "            # Prepare the raw response for later parsing\n",
    "            raw_response = output[\"answer\"].content.strip()\n",
    "            qna_context_list.append([user_input, raw_response, retrieved_contexts])\n",
    "\n",
    "            dataset_df = pd.DataFrame(qna_context_list, columns=[\"user_input\", \"raw_response\", \"retrieved_contexts\"])\n",
    "\n",
    "        else:\n",
    "            response = output[\"answer\"].content\n",
    "            qna_context_list.append([user_input, response, retrieved_contexts])\n",
    "\n",
    "            dataset_df = pd.DataFrame(qna_context_list, columns=[\"user_input\", \"response\", \"retrieved_contexts\"])\n",
    "    \n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_CoT_raw_response_df(df):\n",
    "    processed_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        user_input = row['user_input']\n",
    "        raw_response = row['raw_response']\n",
    "        retrieved_contexts = row['retrieved_contexts']\n",
    "        \n",
    "        # Clean the raw response string\n",
    "        clean_response = raw_response.replace(\"\\n\", \"\").replace(\"\\r\", \"\").replace(\"\\t\", \"\").strip()\n",
    "        \n",
    "        try:\n",
    "            # Try to parse as JSON directly first\n",
    "            if clean_response.startswith(\"```json\"):\n",
    "                # Remove ```json and ``` markers\n",
    "                json_str = clean_response.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "                json_data = json.loads(json_str)\n",
    "            else:\n",
    "                json_data = json.loads(clean_response)\n",
    "            thought = json_data[\"thought\"]\n",
    "            response = json_data[\"answer\"]\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            # If direct parsing fails, try to extract JSON structure\n",
    "            try:\n",
    "                # Find the last occurrence of {\"thought\"\n",
    "                thought_start = clean_response.rindex('{\"thought\"')\n",
    "                # Find the matching closing brace\n",
    "                brace_count = 0\n",
    "                for i in range(thought_start, len(clean_response)):\n",
    "                    if clean_response[i] == '{':\n",
    "                        brace_count += 1\n",
    "                    elif clean_response[i] == '}':\n",
    "                        brace_count -= 1\n",
    "                        if brace_count == 0:\n",
    "                            json_str = clean_response[thought_start:i+1]\n",
    "                            break\n",
    "                \n",
    "                json_data = json.loads(json_str)\n",
    "                thought = json_data[\"thought\"]\n",
    "                response = json_data[\"answer\"]\n",
    "                \n",
    "            except (ValueError, json.JSONDecodeError, KeyError) as e:\n",
    "                print(f\"Failed to parse JSON at index {idx}\")\n",
    "                print(f\"Raw response: {raw_response}\")\n",
    "                # Skip this row or add placeholder values\n",
    "                thought = \"ERROR: Failed to parse thought\"\n",
    "                response = \"ERROR: Failed to parse response\"\n",
    "        \n",
    "        processed_data.append([user_input, thought, response, retrieved_contexts])\n",
    "    \n",
    "    return pd.DataFrame(processed_data, columns=[\"user_input\", \"thought\", \"response\", \"retrieved_contexts\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_openAI = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, api_key=api_key_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_llama = ChatGroq(model=\"llama3-8b-8192\", temperature=0.3, model_kwargs={\"top_p\": 0.9}, api_key=api_key_groq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize prompt and response generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# ========================           BASELINE PROMPT        ============================\n",
    "# ======================================================================================\n",
    "baseline_prompt = PromptTemplate.from_template(\"\"\"HUMAN\\n\n",
    "                                               You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\n",
    "                                               Question: {question}\\n \n",
    "                                               Context: {context}\\n \n",
    "                                               Answer:\n",
    "                                               \"\"\")\n",
    "\n",
    "baseline_response_generator_gpt = ResponseGenerator(prompt=baseline_prompt, llm=llm_openAI)\n",
    "baseline_response_generator_llama = ResponseGenerator(prompt=baseline_prompt, llm=llm_llama)\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# ========================        CHAIN OF THOUGHT PROMPT        =======================\n",
    "# ======================================================================================\n",
    "CoT_prompt = PromptTemplate.from_template(\"\"\"HUMAN\\n\n",
    "                                        You are a world class assistant for legal case question-answering tasks. Think step by step on each of retrieved context on how they help answer the question. Think how different terminologies or party names or entity names in the contexts are related to the question. Think if all the contexts is relevant to the question and ONLY use relevant information to answer the question.\\n\n",
    "                                        How to write the final answer: Answer in legal counseling manner: clear up different terminologies or party name between the question and contexts by mentioning the equivalent terms/word/entity/name in the final answer. State ONLY facts and explicitly say if it ONLY IMPLIES something that answer the question in the final answer. Use maximum of three sentences in the final answer. Just say you don't know if you cannot generate meaningful and factual answer\\n\n",
    "                                        Question: {question}\\n\n",
    "                                        Context: {context}\\n\n",
    "                                        Step-by-step reasoning: Let's think step by step on the context.\\n\n",
    "                                        Output Format, without any additional string/text/character: {{\"thought\":\"description: AI thought on the question and context\",\"answer\":\"description: the final answer to the question\"}}\\n\n",
    "                                        Answer: \n",
    "                                        \"\"\")\n",
    "\n",
    "\n",
    "CoT_response_generator_gpt = ResponseGenerator(prompt=CoT_prompt, llm=llm_openAI)\n",
    "CoT_response_generator_llama = ResponseGenerator(prompt=CoT_prompt, llm=llm_llama)\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# ========================        MANUALLY WRITTEN PROMPT        =======================\n",
    "# ======================================================================================\n",
    "manually_written_prompt = PromptTemplate.from_template(\"\"\"### Instruction:\\n\n",
    "                                                       You are an AI assistant specializing in legal contract analysis. Your task is to carefully examine the *provided Retrieved Chunk* and *answer the user's question accurately*.\\n\n",
    "                                                       Follow these guidelines:\\n\n",
    "                                                       \n",
    "                                                       Read the clause carefully. Identify any terms, conditions, or restrictions related to the user's question.\\n\n",
    "                                                       Answer explicitly based on the clause. If the clause clearly states the information being asked, explain it clearly and accurately.\\n\n",
    "                                                       Do not ignore relevant details. If the clause contains conditions, restrictions, or exceptions, **mention them in your answer.\\n\n",
    "                                                       If the clause does not provide a direct answer, say so. Do not assume or infer information that is not stated.\\n\n",
    "                                                       Support your answer with key phrases from the clause clause when necessary.\\n\n",
    "                                                       Avoid unnecessary repetition or legal jargon. The goal is to make the answer **clear and understandable.\\n\n",
    "                                                       \n",
    "                                                       ### Retrieved Chunk:\\n\n",
    "                                                       {context}\\n\n",
    "                                                       \n",
    "                                                       ### User's Question:\\n\n",
    "                                                       {question}\\n\n",
    "                                                       ### Answer:\\n                                                  \n",
    "                                                       \"\"\")\n",
    "\n",
    "manually_written_response_generator_gpt = ResponseGenerator(prompt=manually_written_prompt, llm=llm_openAI)\n",
    "manually_written_response_generator_llama = ResponseGenerator(prompt=manually_written_prompt, llm=llm_llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate all responses at all K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses_for_k(sample_size, k_values, model_name, baseline_response_generator, CoT_response_generator, manually_written_response_generator):\n",
    "        \n",
    "    for k in k_values:\n",
    "        # Baseline\n",
    "        query_answer_baseline = generate_response_with_context_at_K(baseline_response_generator, sample_size, k_context=k)\n",
    "        query_answer_baseline.to_json(f'query_answer_baseline_{model_name}_k{k}.json', orient=\"records\", indent=4)\n",
    "\n",
    "        # Chain of Thought\n",
    "        query_answer_CoT_raw = generate_response_with_context_at_K(CoT_response_generator, sample_size, JSON_CoT=True, k_context=k)\n",
    "        query_answer_CoT_raw.to_json(f'query_answer_CoT_raw_{model_name}_k{k}.json', orient=\"records\", indent=4)\n",
    "\n",
    "        # Manually written\n",
    "        query_answer_manually_written = generate_response_with_context_at_K(manually_written_response_generator, sample_size, k_context=k)\n",
    "        query_answer_manually_written.to_json(f'query_answer_manually_written_{model_name}_k{k}.json', orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean CoT JSON output\n",
    "\n",
    "def clean_CoT_json_output(k_values, model_name):\n",
    "    for k in k_values:\n",
    "        try:\n",
    "            query_answer_CoT_raw_from_file = pd.read_json(f'query_answer_CoT_raw_{model_name}_k{k}.json')\n",
    "            query_answer_CoT = process_CoT_raw_response_df(query_answer_CoT_raw_from_file)\n",
    "            query_answer_CoT.to_json(f'query_answer_CoT_{model_name}_k{k}.json', orient=\"records\", indent=4)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File query_answer_CoT_raw_{model_name}_k{k}.json not found\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing file for {model_name} k={k}: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error processing file for {model_name} k={k}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate for GPT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses for k values [1, 3, 5, 10] GPT MODEL\n",
    "generate_responses_for_k(sample_size = 192,\n",
    "                         k_values = [1, 3, 5, 10], \n",
    "                         model_name = \"privacyqa_gpt4omini\",\n",
    "                         baseline_response_generator = baseline_response_generator_gpt, \n",
    "                         CoT_response_generator = CoT_response_generator_gpt, \n",
    "                         manually_written_response_generator = manually_written_response_generator_gpt\n",
    "                         )\n",
    "\n",
    "# clean raw CoT JSON output to clean output\n",
    "clean_CoT_json_output([1, 3, 5, 10], \"privacyqa_gpt4omini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate for LLAMA 3 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses for k values [1, 3, 5, 10] GPT MODEL\n",
    "\n",
    "# Test using small value first!!!\n",
    "# e.g. sample size = 5\n",
    "# k_values = [3]\n",
    "\n",
    "generate_responses_for_k(sample_size = 194,\n",
    "                         k_values = [1, 3, 5, 10], \n",
    "                         model_name = \"llama3\",\n",
    "                         baseline_response_generator = baseline_response_generator_llama, \n",
    "                         CoT_response_generator = CoT_response_generator_llama, \n",
    "                         manually_written_response_generator = manually_written_response_generator_llama\n",
    "                         )\n",
    "\n",
    "# clean raw CoT JSON output to clean output\n",
    "clean_CoT_json_output([1, 3, 5, 10], \"llama3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "response_gen_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
