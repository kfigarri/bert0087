{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset    \n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import logging\n",
    "import transformers\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "bert_model = AutoModelForPreTraining.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all file as DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './data/final_generation/privacyqa/'\n",
    "\n",
    "dfs_dict = {}\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.json') and file_name.startswith('query_answer'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            dfs_dict[file_name] = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in dfs_dict:\n",
      "query_answer_human_tuned_privacyqa_gpt4omini_k10.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Files in dfs_dict:\")\n",
    "for file_name in dfs_dict.keys():\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in cot_dfs_dict:\n",
      "query_answer_CoT_llama3_k1.json\n",
      "query_answer_CoT_llama3_k10.json\n",
      "query_answer_CoT_llama3_k3.json\n",
      "query_answer_CoT_llama3_k5.json\n"
     ]
    }
   ],
   "source": [
    "# # Filter for only CoT files\n",
    "# cot_dfs_dict = {k: v for k, v in dfs_dict.items() if 'cot' in k.lower()}\n",
    "\n",
    "# print(\"Files in cot_dfs_dict:\")\n",
    "# for file_name in cot_dfs_dict.keys():\n",
    "#     print(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_dict = cot_dfs_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate using RAGAS + BERT F1 + Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_file(file_index, num_queries = -1):\n",
    "    file_name = list(dfs_dict.keys())[file_index]\n",
    "\n",
    "    if num_queries == -1:\n",
    "        num_queries = dfs_dict[file_name].shape[0]\n",
    "\n",
    "    cur_df = dfs_dict[file_name].head(num_queries)  # Process only the specified number of queries\n",
    "    print(f\"Evaluating file: {file_name}\")\n",
    "\n",
    "    # RAGAS evaluation\n",
    "    evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5))\n",
    "    dataset_ragas = Dataset.from_pandas(cur_df) \n",
    "    # display(cur_df)   \n",
    "    ragas_result = evaluate(\n",
    "        dataset_ragas,\n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy\n",
    "        ],\n",
    "        llm=evaluator_llm\n",
    "    )\n",
    "\n",
    "    ragas_result_dict = ragas_result.__repr__()     \n",
    "\n",
    "    # BERT and ROUGE evaluation\n",
    "    responses = cur_df['response'].tolist()\n",
    "    contexts = cur_df['retrieved_contexts'].tolist()\n",
    "\n",
    "    # Initialize scorers\n",
    "    bert_scorer_obj = BERTScorer(lang=\"en\", model_type=\"nlpaueb/legal-bert-base-uncased\", num_layers=12)\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "    bert_f1_scores = []\n",
    "    rouge_recall_scores = []\n",
    "\n",
    "    for response, context_list in zip(responses, contexts):\n",
    "        # Join all contexts into single reference\n",
    "        if isinstance(context_list, str):\n",
    "            single_ref = context_list\n",
    "        else:\n",
    "            single_ref = \" \".join(context_list)\n",
    "\n",
    "        # Calculate BERT score\n",
    "        _, _, F_mul = bert_scorer_obj.score([response], [single_ref])\n",
    "        bert_f1_scores.append(F_mul.mean().item())\n",
    "\n",
    "        # Calculate ROUGE score\n",
    "        rouge_scores = rouge_scorer_obj.score(single_ref, response)\n",
    "        avg_recall = (rouge_scores[\"rouge1\"].recall + rouge_scores[\"rouge2\"].recall + rouge_scores[\"rougeL\"].recall) / 3\n",
    "        rouge_recall_scores.append(avg_recall)\n",
    "\n",
    "    # Calculate average scores\n",
    "    avg_bert_f1 = sum(bert_f1_scores) / len(bert_f1_scores)\n",
    "    avg_rouge_recall = sum(rouge_recall_scores) / len(rouge_recall_scores)\n",
    "\n",
    "    results = {\n",
    "        'ragas_metrics': ragas_result_dict,  # Use the raw result instead of converting to dict\n",
    "        'bert_f1': avg_bert_f1,\n",
    "        'rouge_recall': avg_rouge_recall\n",
    "    }\n",
    "\n",
    "    results_path = os.path.join(folder_path, \"evaluation_results.json\")\n",
    "\n",
    "    # Load existing results if the file exists\n",
    "    if os.path.exists(results_path):\n",
    "        with open(results_path, \"r\") as results_file:\n",
    "            all_results = json.load(results_file)\n",
    "    else:\n",
    "        all_results = {}\n",
    "\n",
    "    # Add the current result to the dictionary\n",
    "    all_results[file_name] = results\n",
    "\n",
    "    # Save the updated results back to the file\n",
    "    with open(results_path, \"w\") as results_file:\n",
    "        json.dump(all_results, results_file, indent=4)\n",
    "\n",
    "    print(f\"Results for {file_name}:\")\n",
    "    print(f\"RAGAS Metrics: {ragas_result_dict}\")\n",
    "    print(f\"Average BERT F1 Score: {avg_bert_f1}\")\n",
    "    print(f\"Average ROUGE Recall Score: {avg_rouge_recall}\")\n",
    "    print(f\"Results saved to {results_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating file: query_answer_human_tuned_privacyqa_gpt4omini_k10.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4493e820d264476b9a76d6a7c3181833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_file_count = len(dfs_dict)  # Calculate total_file_count from dfs_dict\n",
    "for file_index in range(0, total_file_count):\n",
    "    evaluate_file(file_index=file_index, num_queries=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reorder evaluation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def reorder_evaluation_results(file_path):\n",
    "    # Read the JSON file\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Create a sorted dictionary to store reorganized results\n",
    "    organized_results = {\n",
    "        'baseline': {},\n",
    "        'CoT': {},\n",
    "        'manually_written': {}\n",
    "    }\n",
    "    \n",
    "    # Process each file result\n",
    "    for filename, metrics in data.items():\n",
    "        # Extract k value from filename\n",
    "        if '_k' in filename:\n",
    "            k_value = int(filename.split('_k')[-1].split('.')[0])\n",
    "            k_key = f'k{k_value}'\n",
    "            \n",
    "            # Determine the type and store in organized format\n",
    "            if 'baseline' in filename:\n",
    "                organized_results['baseline'][k_key] = metrics\n",
    "            elif 'CoT' in filename:\n",
    "                organized_results['CoT'][k_key] = metrics\n",
    "            elif 'manually_written' in filename:\n",
    "                organized_results['manually_written'][k_key] = metrics\n",
    "    \n",
    "    # Sort k-values within each type\n",
    "    for response_type in organized_results:\n",
    "        organized_results[response_type] = dict(sorted(\n",
    "            organized_results[response_type].items(),\n",
    "            key=lambda x: int(x[0][1:])  # Sort by k value\n",
    "        ))\n",
    "    \n",
    "    # Write the reorganized results\n",
    "    output_path = file_path.replace('.json', '_reorganized_by_type.json')\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(organized_results, f, indent=4)\n",
    "    \n",
    "    return organized_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline': {'k1': {'ragas_metrics': \"{'faithfulness': 0.6575, 'answer_relevancy': 0.2998}\",\n",
       "   'bert_f1': 0.7057263875745007,\n",
       "   'rouge_recall': 0.3545316826950978},\n",
       "  'k3': {'ragas_metrics': \"{'faithfulness': 0.6912, 'answer_relevancy': 0.3397}\",\n",
       "   'bert_f1': 0.7120289350907827,\n",
       "   'rouge_recall': 0.20389324070405535},\n",
       "  'k5': {'ragas_metrics': \"{'faithfulness': 0.7091, 'answer_relevancy': 0.3539}\",\n",
       "   'bert_f1': 0.710056027493526,\n",
       "   'rouge_recall': 0.15817795055722808},\n",
       "  'k10': {'ragas_metrics': \"{'faithfulness': 0.6748, 'answer_relevancy': 0.2473}\",\n",
       "   'bert_f1': 0.6874822971132613,\n",
       "   'rouge_recall': 0.09626729537838494}},\n",
       " 'CoT': {},\n",
       " 'manually_written': {'k1': {'ragas_metrics': \"{'faithfulness': 0.5012, 'answer_relevancy': 0.2816}\",\n",
       "   'bert_f1': 0.7036664089591232,\n",
       "   'rouge_recall': 0.4897746614800545},\n",
       "  'k3': {'ragas_metrics': \"{'faithfulness': 0.5979, 'answer_relevancy': 0.3799}\",\n",
       "   'bert_f1': 0.7126831827089959,\n",
       "   'rouge_recall': 0.2788349592289338},\n",
       "  'k5': {'ragas_metrics': \"{'faithfulness': 0.6127, 'answer_relevancy': 0.3979}\",\n",
       "   'bert_f1': 0.7143007083651945,\n",
       "   'rouge_recall': 0.20956284003014675},\n",
       "  'k10': {'ragas_metrics': \"{'faithfulness': 0.5364, 'answer_relevancy': 0.4420}\",\n",
       "   'bert_f1': 0.6996896371399004,\n",
       "   'rouge_recall': 0.12434272486469516}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_contractnli = \"data/final_generation/contractnli/evaluation_results.json\"\n",
    "file_path_cuad = \"data/final_generation/cuad/evaluation_results.json\"\n",
    "file_path_maud = \"data/final_generation/maud/evaluation_results.json\"\n",
    "file_path_privacyqa = \"data/final_generation/privacyqa/evaluation_results.json\"\n",
    "\n",
    "\n",
    "reorder_evaluation_results(file_path_contractnli)\n",
    "reorder_evaluation_results(file_path_cuad)\n",
    "reorder_evaluation_results(file_path_maud)\n",
    "reorder_evaluation_results(file_path_privacyqa)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "response_gen_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
