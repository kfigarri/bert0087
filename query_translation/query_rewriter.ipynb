{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use sample: True\n",
      "dataset: contractnli\n",
      "vector store: ./vectorstore/faiss_store_sample_contractnli\n",
      "corpus path: ../data/sample_corpus/contractnli\n",
      "test file: ../data/sample_benchmarks/contractnli.json\n"
     ]
    }
   ],
   "source": [
    "# Universal parameters\n",
    "_use_sample = True\n",
    "dataset_name = \"contractnli\"\n",
    "vectorstore_path = f\"./vectorstore/faiss_store_{'sample_' if _use_sample else ''}{dataset_name}\"\n",
    "directory_path = f\"../data/{'sample_' if _use_sample else ''}corpus/{dataset_name}\"\n",
    "test_file = f\"../data/{'sample_' if _use_sample else ''}benchmarks/{dataset_name}.json\"\n",
    "rephrased_file = f\"../data/{'sample_' if _use_sample else ''}benchmarks/{dataset_name}_rephrased.json\"\n",
    "\n",
    "print(f\"use sample: {_use_sample}\")\n",
    "print(f\"dataset: {dataset_name}\")\n",
    "print(f\"vector store: {vectorstore_path}\")\n",
    "print(f\"corpus path: {directory_path}\")\n",
    "print(f\"test file: {test_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Tuple\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class QASnippet(BaseModel):\n",
    "    file_path: str\n",
    "    span: Tuple[int, int]\n",
    "    answer: str\n",
    "\n",
    "class QAGroundTruth(BaseModel):\n",
    "    query: str\n",
    "    snippets: List[QASnippet]\n",
    "\n",
    "def load_groundtruth(json_file_path: str) -> List[QAGroundTruth]:\n",
    "    \"\"\"\n",
    "    Loads the QA ground-truth data from a JSON file.\n",
    "    Expected JSON format:\n",
    "    {\n",
    "        \"tests\": [\n",
    "            {\n",
    "                \"query\": \"Your query...\",\n",
    "                \"snippets\": [\n",
    "                    {\n",
    "                        \"file_path\": \"path/to/file.txt\",\n",
    "                        \"span\": [start, end],\n",
    "                        \"answer\": \"The answer text...\"\n",
    "                    },\n",
    "                    ...\n",
    "                ]\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    groundtruth_tests = []\n",
    "\n",
    "    try:\n",
    "        tests = data.get(\"tests\")\n",
    "    except Exception as e:\n",
    "        tests = data\n",
    "    \n",
    "    for test in tests:\n",
    "        snippets = [QASnippet(**snippet) for snippet in test[\"snippets\"]]\n",
    "        groundtruth_tests.append(QAGroundTruth(query=test[\"query\"], snippets=snippets))\n",
    "    return groundtruth_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 461 document chunks with spans.\n",
      "FAISS vector store saved locally at './vectorstore/faiss_store_sample_privacy_qa'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\") \n",
    "\n",
    "# sentence-transformers/all-MiniLM-L6-v2\n",
    "# Linq-AI-Research/Linq-Embed-Mistral\n",
    "# thenlper/gte-base\n",
    "\n",
    "def load_documents_with_spans(directory: str, chunk_size: int = 1000, chunk_overlap: int = 0):\n",
    "    \"\"\"\n",
    "    Loads .txt files from a directory, splits each document's text into chunks using\n",
    "    RecursiveCharacterTextSplitter, computes the span (start, end) for each chunk, and\n",
    "    returns a list of Document objects with metadata (including filename, source, and span).\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    # Initialize the splitter with the desired separators and parameters.\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"!\", \"?\", \".\", \":\", \";\", \",\", \" \", \"\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "        strip_whitespace=False,\n",
    "    )\n",
    "    \n",
    "    # Process each .txt file in the directory.\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Split text into chunks.\n",
    "            text_splits = splitter.split_text(text)\n",
    "            \n",
    "            # Verify that the chunks concatenate to the original text.\n",
    "            assert \"\".join(text_splits) == text, \"Concatenated splits do not match the original text.\"\n",
    "            \n",
    "            # Compute spans and create Document objects.\n",
    "            prev_index = 0\n",
    "            for i, chunk_text in enumerate(text_splits):\n",
    "                span = (prev_index, prev_index + len(chunk_text))\n",
    "                prev_index += len(chunk_text)\n",
    "                doc = Document(\n",
    "                    page_content=chunk_text,\n",
    "                    metadata={\n",
    "                        \"filename\": filename,\n",
    "                        \"filepath\": f\"{dataset_name}/{filename}\",\n",
    "                        \"span\": span,  # Stores the (start, end) positions of the chunk.\n",
    "                        \"id\": f\"{filename}_chunk_{i}\"\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "\n",
    "if os.path.exists(vectorstore_path):\n",
    "    print(f\"The vectorstore_path '{vectorstore_path}' already exists. Please delete it first if you wish to continue.\")\n",
    "else:\n",
    "    # Load the documents, splitting each into chunks with span metadata.\n",
    "    documents = load_documents_with_spans(directory_path, chunk_size=500, chunk_overlap=0)\n",
    "    print(f\"Loaded {len(documents)} document chunks with spans.\")\n",
    "\n",
    "    # Build the FAISS vector store using the list of Document objects.\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "    # Save the FAISS vector store locally for later retrieval.\n",
    "    vectorstore.save_local(vectorstore_path)\n",
    "    print(f\"FAISS vector store saved locally at '{vectorstore_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check 5 first samples\n",
    "loaded_vectorstore = FAISS.load_local(vectorstore_path, embeddings, allow_dangerous_deserialization=True)\n",
    "docstore_ids = dict(loaded_vectorstore.index_to_docstore_id)\n",
    "loaded_vectorstore.get_by_ids(list(docstore_ids.values())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DELETE A VECTOR STORE, RUN THIS CELL ##\n",
    "\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Check if the directory exists\n",
    "# if os.path.exists(vectorstore_path):\n",
    "#     shutil.rmtree(vectorstore_path)\n",
    "#     print(f\"Deleted the FAISS vector store at: {vectorstore_path}\")\n",
    "# else:\n",
    "#     print(f\"No FAISS vector store found at: {vectorstore_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Rewriter (Simple Extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/figarrikeisha/.virtualenvs/nlpenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import difflib\n",
    "from collections import Counter\n",
    "from rapidfuzz import fuzz\n",
    "from typing import List, Tuple, Callable\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def extract_tgt_corpus(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the target corpus description from the query and preprocesses it\n",
    "    by removing the term \"Non-Disclosure Agreement\" (case-insensitive) and common English stopwords.\n",
    "    \n",
    "    For example:\n",
    "      \"Consider the Non-Disclosure Agreement between Artop and Inno; Does the document permit...\"\n",
    "    returns (after preprocessing):\n",
    "      \"between Artop Inno\"\n",
    "    \"\"\"\n",
    "    # Extract text between \"Consider the \" and the first semicolon\n",
    "    pattern = r\"^Consider (.*?);\"\n",
    "    match = re.match(pattern, query)\n",
    "    if not match:\n",
    "        return \"\"\n",
    "    \n",
    "    tgt = match.group(1).strip()\n",
    "    \n",
    "    # Remove the term \"Non-Disclosure Agreement\" (case-insensitive)\n",
    "    tgt = re.sub(r\"(?i)Non-Disclosure Agreement\", \"\", tgt).strip()\n",
    "    \n",
    "    # Load common English stopwords from NLTK\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Tokenize the text (here using simple whitespace splitting)\n",
    "    tokens = tgt.split()\n",
    "    \n",
    "    # Filter out stopwords\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    processed_tgt = \" \".join(filtered_tokens)\n",
    "    return processed_tgt\n",
    "\n",
    "def find_best_corpus(tgt_corpus: str, corpus_files: List[str]) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Given a target corpus description and a list of corpus file names,\n",
    "    returns the file name with the highest similarity score and that score.\n",
    "    \"\"\"\n",
    "    best_match = None\n",
    "    best_ratio = 0.0\n",
    "    for file in corpus_files:\n",
    "        ratio = difflib.SequenceMatcher(None, tgt_corpus.lower(), file.lower()).ratio()\n",
    "        if ratio > best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_match = file\n",
    "    return best_match, best_ratio\n",
    "\n",
    "def find_best_corpus_rapid(tgt_corpus: str, corpus_files: List[str]) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Uses RapidFuzz's token_set_ratio to compute a similarity score between the target corpus and each file name.\n",
    "    Returns the best matching file and its score (normalized between 0 and 1).\n",
    "    \"\"\"\n",
    "    best_match = None\n",
    "    best_score = 0.0\n",
    "    for file in corpus_files:\n",
    "        # token_set_ratio handles unordered tokens and common token removal well.\n",
    "        score = fuzz.token_set_ratio(tgt_corpus, file)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = file\n",
    "    # Normalize the score to [0, 1] (RapidFuzz returns a value in [0,100])\n",
    "    return best_match, best_score / 100.0\n",
    "\n",
    "def find_best_corpus_embeddings(tgt_corpus: str, corpus_files: List[str],\n",
    "                                model: SentenceTransformer) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Embeds the target corpus description and each file name using a sentence transformer,\n",
    "    then computes cosine similarities to find the best matching file.\n",
    "    \"\"\"\n",
    "    # Embed the target description.\n",
    "    tgt_embedding = model.encode(tgt_corpus, convert_to_tensor=True)\n",
    "    # Embed all candidate file names.\n",
    "    file_embeddings = model.encode(corpus_files, convert_to_tensor=True)\n",
    "    # Compute cosine similarities.\n",
    "    cosine_scores = util.cos_sim(tgt_embedding, file_embeddings)[0]\n",
    "    # Get the index of the best matching file.\n",
    "    best_idx = int(cosine_scores.argmax())\n",
    "    best_score = float(cosine_scores[best_idx])\n",
    "    return corpus_files[best_idx], best_score\n",
    "\n",
    "\n",
    "def evaluate_corpus_matching(ground_truths: List[QAGroundTruth],\n",
    "                             candidate_files: List[str],\n",
    "                             threshold: float,\n",
    "                             match_fn: Callable[[str, List[str]], Tuple[str, float]]\n",
    "                             ) -> List[int]:\n",
    "    \"\"\"\n",
    "    For each QAGroundTruth:\n",
    "      - Extract the target corpus from the query.\n",
    "      - Find the best matching file using the provided match_fn and its similarity score.\n",
    "      - If the similarity score is below the threshold, assign a score of 0.\n",
    "      - If above the threshold:\n",
    "          * Assign 1 if the best matching file is among the actual file paths.\n",
    "          * Assign -1 if it does not match.\n",
    "    Returns a list of scores.\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for gt in tqdm(ground_truths, desc=\"Evaluating queries\"):\n",
    "        tgt_corpus = extract_tgt_corpus(gt.query)\n",
    "        best_file, similarity = match_fn(tgt_corpus, candidate_files)\n",
    "        # Get the set of actual file paths from the ground truth snippets.\n",
    "        actual_files = {snippet.file_path for snippet in gt.snippets}\n",
    "        \n",
    "        if similarity >= threshold:\n",
    "            score = 1 if best_file in actual_files else -1\n",
    "        else:\n",
    "            score = 0\n",
    "\n",
    "        result = {\n",
    "            \"query\": gt.query,\n",
    "            \"targeted_corpus\": tgt_corpus,\n",
    "            \"best_file\": best_file,\n",
    "            \"similarity\": similarity,\n",
    "            \"actual_files\": list(actual_files),\n",
    "            \"score\": score\n",
    "        }\n",
    "        outputs.append(result)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrased_file = f\"../data/{'sample_' if _use_sample else ''}benchmarks/{dataset_name}_rephrased_split_3.json\"\n",
    "groundtruth_tests = load_groundtruth(rephrased_file) # test_file, rephrased_file\n",
    "test_queries = [gt.query for gt in groundtruth_tests]\n",
    "list_corpus = [os.path.join(f\"{dataset_name}\", filename) for filename in os.listdir(directory_path) if filename.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 60/60 [00:01<00:00, 33.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"query\": \"In legal agreements regarding confidentiality, is it typically stated whether the Receiving Party must inform the Disclosing Party if legally obligated to disclose any Confidential Information?\",\n",
      "  \"targeted_corpus\": \"\",\n",
      "  \"best_file\": \"contractnli/HNBA-2017-18-Confidentiality-Agreement.txt\",\n",
      "  \"similarity\": 0.14289790391921997,\n",
      "  \"actual_files\": [\n",
      "    \"contractnli/Eskom%20Template%20Confidentiality%20and%20Non-disclosure%20Agreement%20Rev%204%20Effective%20August%202017_11.txt\"\n",
      "  ],\n",
      "  \"score\": 0\n",
      "}\n",
      "{\n",
      "  \"query\": \"\\\"Are Non-Disclosure Agreements generally structured to permit the sharing of certain Confidential Information with external parties like consultants, agents, and professional advisors by the Receiving Party?\\\"\",\n",
      "  \"targeted_corpus\": \"\",\n",
      "  \"best_file\": \"contractnli/HNBA-2017-18-Confidentiality-Agreement.txt\",\n",
      "  \"similarity\": 0.14289790391921997,\n",
      "  \"actual_files\": [\n",
      "    \"contractnli/INFOMAGNET%20NDA.txt\"\n",
      "  ],\n",
      "  \"score\": 0\n",
      "}\n",
      "{\n",
      "  \"query\": \"\\\"Are Non-Disclosure Agreements typically designed to limit the use of Confidential Information to specific purposes outlined in the agreement?\\\"\",\n",
      "  \"targeted_corpus\": \"\",\n",
      "  \"best_file\": \"contractnli/HNBA-2017-18-Confidentiality-Agreement.txt\",\n",
      "  \"similarity\": 0.14289790391921997,\n",
      "  \"actual_files\": [\n",
      "    \"contractnli/DBT%20Mutual%20NDA.txt\"\n",
      "  ],\n",
      "  \"score\": 0\n",
      "}\n",
      "{\n",
      "  \"query\": \"\\\"Are clauses in confidentiality agreements generally included to restrict the disclosure of information regarding the agreement's formation or negotiation?\\\"\",\n",
      "  \"targeted_corpus\": \"\",\n",
      "  \"best_file\": \"contractnli/HNBA-2017-18-Confidentiality-Agreement.txt\",\n",
      "  \"similarity\": 0.14289790391921997,\n",
      "  \"actual_files\": [\n",
      "    \"contractnli/Eskom%20Template%20Confidentiality%20and%20Non-disclosure%20Agreement%20Rev%204%20Effective%20August%202017_11.txt\"\n",
      "  ],\n",
      "  \"score\": 0\n",
      "}\n",
      "{\n",
      "  \"query\": \"\\\"Would a typical Non-Disclosure Agreement specify that Confidential Information is limited to technical details?\\\"\",\n",
      "  \"targeted_corpus\": \"\",\n",
      "  \"best_file\": \"contractnli/HNBA-2017-18-Confidentiality-Agreement.txt\",\n",
      "  \"similarity\": 0.14289790391921997,\n",
      "  \"actual_files\": [\n",
      "    \"contractnli/ExcelerateStandardNDAFormat.txt\"\n",
      "  ],\n",
      "  \"score\": 0\n",
      "}\n",
      "dataset:  contractnli\n",
      "Final Score:  Counter({0: 60})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.3\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "match_fn_embeddings = lambda tgt, files: find_best_corpus_embeddings(tgt, files, model)\n",
    "\n",
    "results = evaluate_corpus_matching(groundtruth_tests, list_corpus, threshold, match_fn_embeddings)\n",
    "\n",
    "for sample in random.sample(results, 5):\n",
    "    print(json.dumps(sample, indent=2))\n",
    "\n",
    "with open(f\"../data/results/query_rewriter/SE_{rephrased_file.split('/')[-1]}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "scores = [results[i][\"score\"] for i in range(len(results))]\n",
    "counts = Counter(scores)\n",
    "print(\"dataset: \", dataset_name)\n",
    "print(\"Final Score: \", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Rewriter (Small Language Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/figarrikeisha/.virtualenvs/nlpenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import difflib\n",
    "from collections import Counter\n",
    "from rapidfuzz import fuzz\n",
    "from typing import List, Tuple, Callable\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def split_question(query: str, model) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Splits a query into two parts using a language model with few-shot prompt engineering.\n",
    "    \n",
    "    The function identifies:\n",
    "      - targeted_corpus: a concise phrase that identifies the relevant document or agreement (e.g., \"Evelozcity's Non-Disclosure Agreement\" or \"EFCA's Non-Disclosure Agreement\").\n",
    "      - original_question: the actual question about that document.\n",
    "    \n",
    "    The prompt provides examples for both semicolon-separated queries and naturally phrased queries.\n",
    "    Output is expected as a JSON object with keys 'targeted_corpus' and 'original_question'.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Split the following query into two parts and output a JSON object with keys 'targeted_corpus' and 'original_question'.\\n\"\n",
    "        \"The targeted_corpus should be a short phrase describing the document or agreement being referenced, and the original_question should be the question part.\\n\\n\"\n",
    "        \"Example 1 (semicolon-delimited):\\n\"\n",
    "        \"Input: \\\"Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document indicate that the Agreement does not grant any rights to the Confidential Information?\\\"\\n\"\n",
    "        \"Output: {\\\"targeted_corpus\\\": \\\"CopAcc and ToP Mentors\\\", \\\"original_question\\\": \\\"Does the document indicate that the Agreement does not grant any rights to the Confidential Information?\\\"}\\n\\n\"\n",
    "        \"Example 2 (natural language):\\n\"\n",
    "        \"Input: \\\"Is the Confidential Information covered in Evelozcity's Non-Disclosure Agreement? Are there any specific examples of technical information that is covered?\\\"\\n\"\n",
    "        \"Output: {\\\"targeted_corpus\\\": \\\"Evelozcity\\\", \\\"original_question\\\": \\\"Does the document state that Confidential Information shall only include technical information?\\\"}\\n\\n\"\n",
    "        \"Example 3 (another natural language example):\\n\"\n",
    "        \"Input: \\\"Does the Data Use Agreement in New York City specify if the Receiving Party must return or destroy Confidential Information upon termination?\\\"\\n\"\n",
    "        \"Output: {\\\"targeted_corpus\\\": \\\"Data Use Agreement in New York City\\\", \\\"original_question\\\": \\\"Does the Data Use Agreement in New York City specify if the Receiving Party must return or destroy Confidential Information upon termination?\\\"}\\n\\n\"\n",
    "        \"Now, split the following query:\\n\"\n",
    "        f\"Input: \\\"{query}\\\"\\n\\n\"\n",
    "        \"Output:\"\n",
    "    )\n",
    "    \n",
    "    # Increase max_new_tokens to allow a longer answer and use sampling.\n",
    "    output = model(prompt, max_new_tokens=250, do_sample=True, temperature=0.5)\n",
    "    generated_text = output[0]['generated_text'].strip()\n",
    "    \n",
    "    # Debug print (optional):\n",
    "    # print(\"Raw generated text:\", generated_text)\n",
    "    \n",
    "    # If the generated text does not start with a curly brace, add them.\n",
    "    if not generated_text.startswith(\"{\"):\n",
    "        # Try to extract the JSON-like part using regex (optional improvement).\n",
    "        json_like = re.search(r\"\\{.*\\}\", generated_text, re.DOTALL)\n",
    "        if json_like:\n",
    "            generated_text = json_like.group(0)\n",
    "        else:\n",
    "            generated_text = \"{\" + generated_text + \"}\"\n",
    "    \n",
    "    try:\n",
    "        result = json.loads(generated_text)\n",
    "        targeted_corpus = result.get(\"targeted_corpus\", \"\").strip()\n",
    "        original_question = result.get(\"original_question\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        # Fallback: if JSON parsing fails, fall back to a heuristic split on the semicolon.\n",
    "        parts = query.split(\";\", 1)\n",
    "        targeted_corpus = parts[0].replace(\"Consider\", \"\").strip() if parts else \"\"\n",
    "        original_question = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "    \n",
    "    return targeted_corpus, original_question\n",
    "\n",
    "def find_best_corpus(tgt_corpus: str, corpus_files: List[str]) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Given a target corpus description and a list of corpus file names,\n",
    "    returns the file name with the highest similarity score and that score.\n",
    "    \"\"\"\n",
    "    best_match = None\n",
    "    best_ratio = 0.0\n",
    "    for file in corpus_files:\n",
    "        ratio = difflib.SequenceMatcher(None, tgt_corpus.lower(), file.lower()).ratio()\n",
    "        if ratio > best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_match = file\n",
    "    return best_match, best_ratio\n",
    "\n",
    "def find_best_corpus_rapid(tgt_corpus: str, corpus_files: List[str]) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Uses RapidFuzz's token_set_ratio to compute a similarity score between the target corpus and each file name.\n",
    "    Returns the best matching file and its score (normalized between 0 and 1).\n",
    "    \"\"\"\n",
    "    best_match = None\n",
    "    best_score = 0.0\n",
    "    for file in corpus_files:\n",
    "        # token_set_ratio handles unordered tokens and common token removal well.\n",
    "        score = fuzz.token_set_ratio(tgt_corpus, file)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = file\n",
    "    # Normalize the score to [0, 1] (RapidFuzz returns a value in [0,100])\n",
    "    return best_match, best_score / 100.0\n",
    "\n",
    "def find_best_corpus_embeddings(tgt_corpus: str, corpus_files: List[str],\n",
    "                                model: SentenceTransformer) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Embeds the target corpus description and each file name using a sentence transformer,\n",
    "    then computes cosine similarities to find the best matching file.\n",
    "    \"\"\"\n",
    "    # Embed the target description.\n",
    "    tgt_embedding = model.encode(tgt_corpus, convert_to_tensor=True)\n",
    "    # Embed all candidate file names.\n",
    "    file_embeddings = model.encode(corpus_files, convert_to_tensor=True)\n",
    "    # Compute cosine similarities.\n",
    "    cosine_scores = util.cos_sim(tgt_embedding, file_embeddings)[0]\n",
    "    # Get the index of the best matching file.\n",
    "    best_idx = int(cosine_scores.argmax())\n",
    "    best_score = float(cosine_scores[best_idx])\n",
    "    return corpus_files[best_idx], best_score\n",
    "\n",
    "\n",
    "def evaluate_corpus_matching(ground_truths: List[QAGroundTruth],\n",
    "                             candidate_files: List[str],\n",
    "                             threshold: float,\n",
    "                             match_fn: Callable[[str, List[str]], Tuple[str, float]]\n",
    "                             ) -> List[int]:\n",
    "    \"\"\"\n",
    "    For each QAGroundTruth:\n",
    "      - Extract the target corpus from the query.\n",
    "      - Find the best matching file using the provided match_fn and its similarity score.\n",
    "      - If the similarity score is below the threshold, assign a score of 0.\n",
    "      - If above the threshold:\n",
    "          * Assign 1 if the best matching file is among the actual file paths.\n",
    "          * Assign -1 if it does not match.\n",
    "    Returns a list of scores.\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    model = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "    for gt in tqdm(ground_truths, desc=\"Evaluating queries\"):\n",
    "        tgt_corpus, orig = split_question(gt.query, model)\n",
    "        best_file, similarity = match_fn(tgt_corpus, candidate_files)\n",
    "        # Get the set of actual file paths from the ground truth snippets.\n",
    "        actual_files = {snippet.file_path for snippet in gt.snippets}\n",
    "        \n",
    "        if similarity >= threshold:\n",
    "            score = 1 if best_file in actual_files else -1\n",
    "        else:\n",
    "            score = 0\n",
    "\n",
    "        result = {\n",
    "            \"query\": gt.query,\n",
    "            \"targeted_corpus\": tgt_corpus,\n",
    "            \"best_file\": best_file,\n",
    "            \"similarity\": similarity,\n",
    "            \"actual_files\": list(actual_files),\n",
    "            \"score\": score\n",
    "        }\n",
    "        outputs.append(result)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrased_file = f\"../data/{'sample_' if _use_sample else ''}benchmarks/{dataset_name}_rephrased_split_2.json\"\n",
    "groundtruth_tests = load_groundtruth(rephrased_file) # test_file, rephrased_file\n",
    "test_queries = [gt.query for gt in groundtruth_tests]\n",
    "list_corpus = [os.path.join(f\"{dataset_name}\", filename) for filename in os.listdir(directory_path) if filename.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Evaluating queries: 100%|██████████| 60/60 [07:27<00:00,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"query\": \"\\\"Consider DBT's Mutual Non-Disclosure Agreement; Does the document include a clause that prevents the Receiving Party from disclosing the fact that the Agreement was agreed upon or negotiated?\\\"\",\n",
      "  \"targeted_corpus\": \"DBT's Mutual Non-Disclosure Agreement; Does the document include a clause that prevents the Receiving Party from disclosing the fact that the Agreement was agreed upon or negotiated?\",\n",
      "  \"best_file\": \"contractnli/IGC-Non-Disclosure-Agreement-LSE-Sample.txt\",\n",
      "  \"similarity\": 0.5515354871749878,\n",
      "  \"actual_files\": [\n",
      "    \"contractnli/DBT%20Mutual%20NDA.txt\"\n",
      "  ],\n",
      "  \"score\": -1\n",
      "}\n",
      "{\n",
      "  \"query\": \"\\\"In the Non-Disclosure Agreement between IGC and LSE, does the document mention whether certain obligations continue even after the Agreement is terminated?\\\"\",\n",
      "  \"targeted_corpus\": \"\\\"In the Non-Disclosure Agreement between IGC and LSE, does the document mention whether certain obligations continue even after the Agreement is terminated?\\\"\",\n",
      "  \"best_file\": \"contractnli/IGC-Non-Disclosure-Agreement-LSE-Sample.txt\",\n",
      "  \"similarity\": 0.6690535545349121,\n",
      "  \"actual_files\": [\n",
      "    \"contractnli/IGC-Non-Disclosure-Agreement-LSE-Sample.txt\"\n",
      "  ],\n",
      "  \"score\": 1\n",
      "}\n",
      "{\n",
      "  \"query\": \"\\\"Consider the Non-Disclosure Agreement between Hochschule Furtwangen University and Thesis Participants; Does the document allow the Receiving Party to share some Confidential Information with third parties, including consultants, agents, and professional advisors??\\\"\",\n",
      "  \"targeted_corpus\": \"\\\" the Non-Disclosure Agreement between Hochschule Furtwangen University and Thesis Participants\",\n",
      "  \"best_file\": \"contractnli/IGC-Non-Disclosure-Agreement-LSE-Sample.txt\",\n",
      "  \"similarity\": 0.4437294006347656,\n",
      "  \"actual_files\": [\n",
      "    \"contractnli/Geheimhaltungsvereinbarung_Abschlussarbeiten_HFU_englisch.txt\"\n",
      "  ],\n",
      "  \"score\": -1\n",
      "}\n",
      "{\n",
      "  \"query\": \"\\\"In APIC's Confidentiality Agreement, what is the statement that the Confidential Information shall include?\",\n",
      "  \"targeted_corpus\": \"APIC's Confidentiality Agreement\",\n",
      "  \"best_file\": \"contractnli/Focus-Group-APIC-Seattle-Confidentiality-Agreement-031115.txt\",\n",
      "  \"similarity\": 0.6528750658035278,\n",
      "  \"actual_files\": [\n",
      "    \"contractnli/Focus-Group-APIC-Seattle-Confidentiality-Agreement-031115.txt\"\n",
      "  ],\n",
      "  \"score\": 1\n",
      "}\n",
      "{\n",
      "  \"query\": \"\\\"Does HNBA's Confidentiality Agreement restrict the use of Confidential Information to the purposes stated in the Agreement?\\\"\",\n",
      "  \"targeted_corpus\": \"HNBA's Confidentiality Agreement\",\n",
      "  \"best_file\": \"contractnli/HNBA-2017-18-Confidentiality-Agreement.txt\",\n",
      "  \"similarity\": 0.7620667219161987,\n",
      "  \"actual_files\": [\n",
      "    \"contractnli/HNBA-2017-18-Confidentiality-Agreement.txt\"\n",
      "  ],\n",
      "  \"score\": 1\n",
      "}\n",
      "dataset:  contractnli\n",
      "Final Score:  Counter({1: 36, -1: 21, 0: 3})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.3\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "match_fn_embeddings = lambda tgt, files: find_best_corpus_embeddings(tgt, files, model)\n",
    "\n",
    "results = evaluate_corpus_matching(groundtruth_tests, list_corpus, threshold, match_fn_embeddings)\n",
    "\n",
    "for sample in random.sample(results, 5):\n",
    "    print(json.dumps(sample, indent=2))\n",
    "\n",
    "scores = [results[i][\"score\"] for i in range(len(results))]\n",
    "counts = Counter(scores)\n",
    "print(\"dataset: \", dataset_name)\n",
    "print(\"Final Score: \", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Rewriter (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/figarrikeisha/.virtualenvs/nlpenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Callable\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def load_groundtruth(json_file_path: str) -> List[QAGroundTruth]:\n",
    "    \"\"\"\n",
    "    Loads the QA ground-truth data from a JSON file.\n",
    "    Expected JSON format:\n",
    "    {\n",
    "        \"tests\": [\n",
    "            {\n",
    "                \"query\": \"Your query...\",\n",
    "                \"snippets\": [\n",
    "                    { \"file_path\": \"path/to/file.txt\", \"span\": [start, end], \"answer\": \"The answer text...\" },\n",
    "                    ...\n",
    "                ]\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    groundtruth_tests = []\n",
    "    tests = data.get(\"tests\", data)\n",
    "    for test in tests:\n",
    "        snippets = [QASnippet(**snippet) for snippet in test[\"snippets\"]]\n",
    "        groundtruth_tests.append(QAGroundTruth(query=test[\"query\"], snippets=snippets))\n",
    "    return groundtruth_tests\n",
    "\n",
    "def split_question_ner(query: str, ner_model) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Splits a query into two parts using Named Entity Recognition.\n",
    "    \n",
    "    If the query contains a semicolon, it uses that to split into:\n",
    "      - targeted_corpus: the text before the semicolon (after removing \"Consider\")\n",
    "      - original_question: the text after the semicolon.\n",
    "      \n",
    "    Otherwise, it uses the provided NER pipeline to extract an organization or miscellaneous entity \n",
    "    (e.g. an agreement or company name) from the query as the targeted corpus.\n",
    "    The remainder of the query (with the extracted entity removed) is taken as the original question.\n",
    "    \n",
    "    Example:\n",
    "      Input: \"Is the Confidential Information covered in Evelozcity's Non-Disclosure Agreement? Are there any specific examples of technical information that is covered?\"\n",
    "      Might yield: (\"Evelozcity\", \"Is the Confidential Information covered? Are there any specific examples of technical information that is covered?\")\n",
    "    \"\"\"\n",
    "    \n",
    "    pattern = r\"^Consider (.*?);\"\n",
    "    match = re.match(pattern, query)\n",
    "    if match:\n",
    "        tgt = match.group(1).strip()\n",
    "        # Remove the term \"Non-Disclosure Agreement\" (case-insensitive)\n",
    "        tgt = re.sub(r\"(?i)Non-Disclosure Agreement\", \"\", tgt).strip()\n",
    "        # Load common English stopwords from NLTK\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        # Tokenize the text (here using simple whitespace splitting)\n",
    "        tokens = tgt.split()\n",
    "        # Filter out stopwords\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "        # Join tokens back into a string\n",
    "        targeted_corpus = \" \".join(filtered_tokens)\n",
    "        # Define original question\n",
    "        parts = query.split(\";\", 1)\n",
    "        original_question = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "        return targeted_corpus, original_question\n",
    "\n",
    "    else:\n",
    "        # Use NER to extract an entity (e.g., organization) as the targeted corpus.\n",
    "        ner_results = ner_model(query)\n",
    "        # Filter for entities with label ORG.\n",
    "        org_entities = [ent[\"word\"] for ent in ner_results if ent.get(\"entity_group\") in [\"ORG\", \"MISC\"] and ent.get(\"score\") > 0.8]\n",
    "        # Exclude common generic words.\n",
    "        filtered_orgs = [org.strip() for org in org_entities if org.strip().lower() not in [\"agreement\", \"nda\", \"non-disclosure\", \"agreements\"]]\n",
    "        \n",
    "        if filtered_orgs:\n",
    "            targeted_corpus = \" \".join(filtered_orgs)\n",
    "        elif org_entities:\n",
    "            targeted_corpus = \" \".join(org_entities)\n",
    "        else:\n",
    "            targeted_corpus = \"\"\n",
    "    \n",
    "    # Remove the targeted_corpus from the query to form the original question.\n",
    "    original_question = query.replace(targeted_corpus, \"\").strip()\n",
    "    return targeted_corpus, original_question\n",
    "    \n",
    "def find_best_corpus_embeddings(tgt_corpus: str, corpus_files: List[str],\n",
    "                                model: SentenceTransformer) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Embeds the target corpus description and each file name using a sentence transformer,\n",
    "    then computes cosine similarities to find the best matching file.\n",
    "    \"\"\"\n",
    "    # Embed the target description.\n",
    "    tgt_embedding = model.encode(tgt_corpus, convert_to_tensor=True)\n",
    "    # Embed all candidate file names.\n",
    "    file_embeddings = model.encode(corpus_files, convert_to_tensor=True)\n",
    "    # Compute cosine similarities.\n",
    "    cosine_scores = util.cos_sim(tgt_embedding, file_embeddings)[0]\n",
    "    # Get the index of the best matching file.\n",
    "    best_idx = int(cosine_scores.argmax())\n",
    "    best_score = float(cosine_scores[best_idx])\n",
    "    return corpus_files[best_idx], best_score\n",
    "\n",
    "def evaluate_corpus_matching(ground_truths: List[QAGroundTruth],\n",
    "                             candidate_files: List[str],\n",
    "                             threshold: float,\n",
    "                             match_fn: Callable[[str, List[str]], Tuple[str, float]],\n",
    "                             split_fn: Callable[[str], Tuple[str, str]]\n",
    "                             ) -> List[dict]:\n",
    "    \"\"\"\n",
    "    For each QAGroundTruth:\n",
    "      - Splits the query into targeted_corpus and original_question using split_fn.\n",
    "      - Finds the best matching file using the provided match_fn and its similarity score.\n",
    "      - If the similarity score is below the threshold, assigns a score of 0.\n",
    "      - If above the threshold:\n",
    "          * Assigns 1 if the best matching file is among the actual file paths.\n",
    "          * Assigns -1 if it does not match.\n",
    "    \n",
    "    Returns a list of dictionaries with the following keys:\n",
    "      - \"query\", \"targeted_corpus\", \"best_file\", \"similarity\", \"actual_files\", and \"score\".\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for gt in tqdm(ground_truths, desc=\"Evaluating queries\"):\n",
    "        targeted_corpus, _ = split_fn(gt.query)\n",
    "        best_file, similarity = match_fn(targeted_corpus, candidate_files)\n",
    "        actual_files = {snippet.file_path for snippet in gt.snippets}\n",
    "        \n",
    "        if similarity >= threshold:\n",
    "            score = 1 if best_file in actual_files else -1\n",
    "        else:\n",
    "            score = 0\n",
    "        \n",
    "        result = {\n",
    "            \"query\": gt.query,\n",
    "            \"targeted_corpus\": targeted_corpus,\n",
    "            \"best_file\": best_file,\n",
    "            \"similarity\": similarity,\n",
    "            \"actual_files\": list(actual_files),\n",
    "            \"score\": score\n",
    "        }\n",
    "        outputs.append(result)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrased_file = f\"../data/{'sample_' if _use_sample else ''}benchmarks/{dataset_name}_split_1.json\"\n",
    "groundtruth_tests = load_groundtruth(rephrased_file) # test_file, rephrased_file\n",
    "test_queries = [gt.query for gt in groundtruth_tests]\n",
    "list_corpus = [os.path.join(f\"{dataset_name}\", filename) for filename in os.listdir(directory_path) if filename.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Evaluating queries: 100%|██████████| 60/60 [00:01<00:00, 30.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"query\": \"Consider Epsteen's Non-Disclosure Agreement; Does the document include a clause that prohibits the Receiving Party from soliciting some of the Disclosing Party's representatives?\",\n",
      "  \"targeted_corpus\": \"Epsteen's\",\n",
      "  \"best_file\": \"contractnli/epsteen_nda.txt\",\n",
      "  \"similarity\": 0.4742737412452698,\n",
      "  \"actual_files\": [\n",
      "    \"contractnli/epsteen_nda.txt\"\n",
      "  ],\n",
      "  \"score\": 1\n",
      "}\n",
      "{\n",
      "  \"query\": \"Consider DBT's Mutual Non-Disclosure Agreement; Does the document allow the Receiving Party to independently develop information that is similar to the Confidential Information?\",\n",
      "  \"targeted_corpus\": \"DBT's Mutual\",\n",
      "  \"best_file\": \"contractnli/DBT%20Mutual%20NDA.txt\",\n",
      "  \"similarity\": 0.42951324582099915,\n",
      "  \"actual_files\": [\n",
      "    \"contractnli/DBT%20Mutual%20NDA.txt\"\n",
      "  ],\n",
      "  \"score\": 1\n",
      "}\n",
      "{\n",
      "  \"query\": \"Consider HNBA's Confidentiality Agreement; Does the document indicate that the Agreement does not grant the Receiving Party any rights to the Confidential Information?\",\n",
      "  \"targeted_corpus\": \"HNBA's Confidentiality Agreement\",\n",
      "  \"best_file\": \"contractnli/HNBA-2017-18-Confidentiality-Agreement.txt\",\n",
      "  \"similarity\": 0.7620667219161987,\n",
      "  \"actual_files\": [\n",
      "    \"contractnli/HNBA-2017-18-Confidentiality-Agreement.txt\"\n",
      "  ],\n",
      "  \"score\": 1\n",
      "}\n",
      "{\n",
      "  \"query\": \"Consider HNBA's Confidentiality Agreement; Does the document allow the Receiving Party to independently develop information that is similar to the Confidential Information?\",\n",
      "  \"targeted_corpus\": \"HNBA's Confidentiality Agreement\",\n",
      "  \"best_file\": \"contractnli/HNBA-2017-18-Confidentiality-Agreement.txt\",\n",
      "  \"similarity\": 0.7620667219161987,\n",
      "  \"actual_files\": [\n",
      "    \"contractnli/HNBA-2017-18-Confidentiality-Agreement.txt\"\n",
      "  ],\n",
      "  \"score\": 1\n",
      "}\n",
      "{\n",
      "  \"query\": \"Consider INFOMAGNET's Non-Disclosure Agreement; Does the document permit the Receiving Party to retain some Confidential Information even after its return or destruction?\",\n",
      "  \"targeted_corpus\": \"INFOMAGNET's\",\n",
      "  \"best_file\": \"contractnli/INFOMAGNET%20NDA.txt\",\n",
      "  \"similarity\": 0.5078193545341492,\n",
      "  \"actual_files\": [\n",
      "    \"contractnli/INFOMAGNET%20NDA.txt\"\n",
      "  ],\n",
      "  \"score\": 1\n",
      "}\n",
      "dataset:  contractnli\n",
      "Final Score:  Counter({1: 58, 0: 2})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.3\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "match_fn_embeddings = lambda tgt, files: find_best_corpus_embeddings(tgt, files, model)\n",
    "\n",
    "# Define the split function to use NER.\n",
    "# Jean-Baptiste/roberta-large-ner-english\n",
    "# dbmdz/bert-large-cased-finetuned-conll03-english\n",
    "ner_model = pipeline(\"ner\", model=\"Jean-Baptiste/roberta-large-ner-english\", aggregation_strategy=\"simple\")\n",
    "split_fn = lambda query: split_question_ner(query, ner_model)\n",
    "\n",
    "results = evaluate_corpus_matching(groundtruth_tests, list_corpus, threshold, match_fn_embeddings, split_fn)\n",
    "\n",
    "with open(f\"../data/results/query_rewriter/NER_{rephrased_file.split('/')[-1]}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "for sample in random.sample(results, 5):\n",
    "    print(json.dumps(sample, indent=2))\n",
    "\n",
    "scores = [results[i][\"score\"] for i in range(len(results))]\n",
    "counts = Counter(scores)\n",
    "print(\"dataset: \", dataset_name)\n",
    "print(\"Final Score: \", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rephrase Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rephrase into natural conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional\n",
    "from transformers import pipeline\n",
    "\n",
    "def rephrase_question(question: str, model, custom_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Rephrases a question into a more natural, real-world style while preserving two distinct parts:\n",
    "      - A part providing details about the relevant document or agreement (targeted corpus).\n",
    "      - The actual query regarding that document.\n",
    "      \n",
    "    The function uses prompt engineering with a few-shot approach. The output is expected to be exactly the rephrased question.\n",
    "    \n",
    "    Few-shot examples:\n",
    "    Example 1:\n",
    "      Original: \"In the Non-Disclosure Agreement between CopAcc and ToP Mentors, does it explicitly state that the Receiving Party is not granted any rights to the Confidential Information?\"\n",
    "      Rephrased: \"In the Non-Disclosure Agreement between CopAcc and ToP Mentors, does it explicitly state that the Receiving Party is not granted any rights to the Confidential Information?\"\n",
    "      \n",
    "    Example 2:\n",
    "      Original: \"Consider EFCA's Non-Disclosure Agreement; Does the document mention that some obligations of the Agreement may survive the termination of the Agreement?\"\n",
    "      Rephrased: \"Does EFCA's Non-Disclosure Agreement mention whether certain obligations continue even after the Agreement is terminated?\"\n",
    "      \n",
    "    Example 3:\n",
    "      Original: \"Consider the Data Use Agreement in New York City; Does the document specify whether the Receiving Party is required to destroy or return Confidential Information upon the termination of the Agreement?\"\n",
    "      Rephrased: \"In the Data Use Agreement for New York City, does the document specify if the Receiving Party must destroy or return Confidential Information once the Agreement ends?\"\n",
    "    \n",
    "    Now rephrase the following question:\n",
    "      Original: \"{question}\"\n",
    "    \n",
    "    Output exactly as:\n",
    "      Rephrased: \"<your rephrased question here>\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use sampling to allow creative rephrasing.\n",
    "    prompt = custom_prompt.format(question=question)\n",
    "    output = model(prompt, max_length=150, do_sample=True, temperature=0.8)\n",
    "    generated_text = output[0]['generated_text']\n",
    "    \n",
    "    # Attempt to parse the output if it follows our exact format\n",
    "    # Here we assume the model's output starts with \"Rephrased:\" and then the text.\n",
    "    if generated_text.strip().lower().startswith(\"rephrased:\"):\n",
    "        rephrased = generated_text.strip()[len(\"Rephrased:\"):].strip()\n",
    "    else:\n",
    "        rephrased = generated_text.strip()\n",
    "    \n",
    "    return rephrased\n",
    "\n",
    "def rephrase_groundtruth_queries(groundtruths: List[QAGroundTruth],\n",
    "                                 model,\n",
    "                                 percentage: float = 0.5,\n",
    "                                 custom_prompt: Optional[str] = None) -> List[QAGroundTruth]:\n",
    "    \"\"\"\n",
    "    Rephrases a given percentage of ground truth queries using the specified model and custom prompt.\n",
    "    \n",
    "    Args:\n",
    "      groundtruths: List of QAGroundTruth objects.\n",
    "      model: A text-to-text generation pipeline.\n",
    "      percentage: Fraction of queries to rephrase.\n",
    "      custom_prompt: A custom prompt string with a {question} placeholder.\n",
    "      \n",
    "    Returns:\n",
    "      The updated list of QAGroundTruth objects with rephrased queries.\n",
    "    \"\"\"\n",
    "    if custom_prompt is None:\n",
    "        # Fallback default prompt.\n",
    "        custom_prompt = (\n",
    "            \"Example 1:\\n\"\n",
    "            \"Original: \\\"Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document indicate that the Agreement does not grant any rights to the Confidential Information?\\\"\\n\"\n",
    "            \"Rephrased: \\\"In the Non-Disclosure Agreement between CopAcc and ToP Mentors, does it explicitly state that the Receiving Party is not granted any rights to the Confidential Information?\\\"\\n\\n\"\n",
    "            \"Example 2:\\n\"\n",
    "            \"Original: \\\"Consider EFCA's Non-Disclosure Agreement; Does the document mention that some obligations of the Agreement may survive the termination of the Agreement?\\\"\\n\"\n",
    "            \"Rephrased: \\\"Does EFCA's Non-Disclosure Agreement mention whether certain obligations continue even after the Agreement is terminated?\\\"\\n\\n\"\n",
    "            \"Example 3:\\n\"\n",
    "            \"Original: \\\"Consider the Data Use Agreement in New York City; Does the document specify whether the Receiving Party is required to destroy or return Confidential Information upon the termination of the Agreement?\\\"\\n\"\n",
    "            \"Rephrased: \\\"In the Data Use Agreement for New York City, does the document specify if the Receiving Party must destroy or return Confidential Information once the Agreement ends?\\\"\\n\\n\"\n",
    "            \"Now, rephrase the following question in a natural, conversational style while preserving the two parts (the document details and the query):\\n\"\n",
    "            \"Original: \\\"{question}\\\"\\n\\n\"\n",
    "            \"Rephrased: \"\n",
    "        )\n",
    "\n",
    "    num_to_rephrase = int(len(groundtruths) * percentage)\n",
    "    indices = random.sample(range(len(groundtruths)), num_to_rephrase)\n",
    "    \n",
    "    for idx in tqdm(indices, desc=\"Rephrasing queries\"):\n",
    "        original_query = groundtruths[idx].query\n",
    "        new_query = rephrase_question(original_query, model, custom_prompt)\n",
    "        groundtruths[idx].query = new_query\n",
    "    return groundtruths\n",
    "\n",
    "def save_groundtruth(groundtruths: List[QAGroundTruth], output_path: str):\n",
    "    \"\"\"\n",
    "    Saves the list of QAGroundTruth objects to a JSON file in the expected format.\n",
    "    \"\"\"\n",
    "    tests = [gt.dict() for gt in groundtruths]\n",
    "    output_data = {\"tests\": tests}\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "    print(f\"Saved rephrased groundtruth data to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruths_sample2 = f\"../data/{'sample_' if _use_sample else ''}benchmarks/{dataset_name}_rephrased_split_3.json\"\n",
    "groundtruths = load_groundtruth(groundtruths_sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Would a typical Non-Disclosure Agreement specify that Confidential Information is limited to technical details?\"\n",
      "\"In legal agreements involving confidential information, do provisions exist that allow the Receiving Party to keep certain Confidential Information even after returning or destroying it?\"\n",
      "\"Within standard agreements, would the Receiving Party typically be permitted to disclose certain Confidential Information to their employees?\"\n",
      "\"Within a typical agreement of this nature, does it typically grant permission for the Receiving Party to disclose Confidential Information to third parties such as consultants, agents, and professional advisors?\"\n",
      "\"Can Confidentiality Agreements typically permit the sharing of Confidential Information with third parties such as consultants, agents, and professional advisors?\"\n"
     ]
    }
   ],
   "source": [
    "for i in random.sample(groundtruths, 5):\n",
    "    print(i.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Rephrasing queries: 100%|██████████| 60/60 [04:07<00:00,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rephrased groundtruth data to ../data/sample_benchmarks/contractnli_rephrased_split_2.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/hk/j9r7jggx4dxgt8gmzj_c2z080000gn/T/ipykernel_17242/441073255.py:95: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  tests = [gt.dict() for gt in groundtruths]\n"
     ]
    }
   ],
   "source": [
    "# Load the ground truth test data from a JSON file.\n",
    "groundtruths_sample2 = f\"../data/{'sample_' if _use_sample else ''}benchmarks/{dataset_name}_split_2.json\"\n",
    "groundtruths = load_groundtruth(groundtruths_sample2)\n",
    "\n",
    "# Load a text-to-text generation pipeline using an open-source model.\n",
    "model = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "\n",
    "# Rephrase a specified percentage (e.g., 50%) of the queries.\n",
    "rephrased_groundtruths = rephrase_groundtruth_queries(groundtruths, model, percentage=1)\n",
    "\n",
    "# Save the updated groundtruth test data.\n",
    "rephrased_file = f\"../data/{'sample_' if _use_sample else ''}benchmarks/{dataset_name}_rephrased_split_2.json\"\n",
    "save_groundtruth(rephrased_groundtruths, rephrased_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groundtruths_rephrased = load_groundtruth(rephrased_file)\n",
    "\n",
    "len(groundtruths_rephrased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[QAGroundTruth(query='\"In INFOMAGNET\\'s Non-Disclosure Agreement is it stated that Confidential Information shall only include technical information?\"', snippets=[QASnippet(file_path='contractnli/INFOMAGNET%20NDA.txt', span=(789, 1138), answer='\"Confidential Information\" includes, without limitation, information relating to released or unreleased Disclosing Party software products, the marketing or promotion of any Disclosing Party product, Disclosing Party\\'s business policies or practices, and information received from others that Disclosing Party is obligated to treat as confidential. '), QASnippet(file_path='contractnli/INFOMAGNET%20NDA.txt', span=(1843, 2063), answer='(c) \"Confidential Materials\" shall mean all tangible materials containing Confidential Information, including without limitation written or printed documents and computer disks or tapes, whether machine or user readable.'), QASnippet(file_path='contractnli/INFOMAGNET%20NDA.txt', span=(5914, 6289), answer='The terms \"residuals\" means information in non-tangible form, which may be retained by persons who have access to the Confidential Information, including ideas, concepts, know-how or techniques contained therein. Neither party shall have any obligation to limit or restrict the assignment of such persons or to pay royalties for any work resulting from the use of residuals. ')]),\n",
       " QAGroundTruth(query='\"Consider the Non-Disclosure Agreement between Hochschule Furtwangen University and Thesis Participants; Does the document allow the Receiving Party to share some Confidential Information with third parties, including consultants, agents, and professional advisors??\"', snippets=[QASnippet(file_path='contractnli/Geheimhaltungsvereinbarung_Abschlussarbeiten_HFU_englisch.txt', span=(2776, 2965), answer='In particular, the university is permitted to reveal confidential information to third parties in as far as this is necessary to allow for the proper performance of the assessment process. ')]),\n",
       " QAGroundTruth(query='\"In the Mutual Non-Disclosure Agreement between IPTK and CO, does the document allow verbally conveyed information to be considered as Confidential Information?\"', snippets=[QASnippet(file_path='contractnli/IPTK-CO-MutualNon-DisclosureAgreement.txt', span=(1141, 2571), answer='a. “Confidential Information” shall mean all confidential, proprietary and trade secret information and materials, whether in written, oral, visually, electronic or another format (including, for example, demonstrations, models or proto-types, software, computer tapes, audio or video tapes or recordings, other media), and whether intentionally disclosed or observed inadvertently, includ-ing, but not limited to, the following: [insert specific description if possible]; research, product plans or other information regarding the Discloser’s prod-ucts or services and markets therefor, customer lists and customers, software, developments, inventions, processes, formulas, technology, designs, drawings, engineering, hardware configuration information, marketing, finances or other business information; information of a confidential, sensitive, non-public, or personal nature, including information belonging to a third party or for which the Company owes a duty of confidentiality; any other Company proprietary or confidential information, including but not limited to any materials or any oral and written communications between the parties marked “confidential,” “proprietary” or similarly marked; or any materials which a reasonable person would recognize from the surrounding facts and circumstances to be proprietary or confidential.\\nb. The Confidential Information that must be protected under this Agreement in-cludes '), QASnippet(file_path='contractnli/IPTK-CO-MutualNon-DisclosureAgreement.txt', span=(2777, 3000), answer='(ii) information in oral or visual form that is identified as being Confidential Information at the time of disclosure and confirmed in writing as Confidential Information within fourteen (14) days after the disclosure; or ')]),\n",
       " QAGroundTruth(query='\"On the Mutual Non-Disclosure Agreement between IPTK and CO, does the document mention that certain obligations of the Agreement may survive the termination of the Agreement?\"', snippets=[QASnippet(file_path='contractnli/IPTK-CO-MutualNon-DisclosureAgreement.txt', span=(6543, 7034), answer='This Agreement applies to any Confidential Information that may have been provided by either Party before or after the Effective Date, and will continue to govern all disclosures of Confidential Information, until terminated on thirty (30) days writ-ten notice by either Party to the other, except that each Party’s obligations relating to Confidential Information disclosed prior to termination will continue for so long as the Confidential Information remains confidential and proprietary.')]),\n",
       " QAGroundTruth(query='\"In the Non-Disclosure Agreement between IGC and LSE, does it explicitly state that the Receiving Party is not granted any rights to the Confidential Information?\"', snippets=[QASnippet(file_path='contractnli/IGC-Non-Disclosure-Agreement-LSE-Sample.txt', span=(10568, 10975), answer=\"4.1 All Confidential Information shall remain the property of the Disclosing Party. Each party reserves all rights in its Confidential Information. No rights, including, but not limited to, intellectual property rights, in respect of a party's Confidential Information are granted to the other party and no obligations are imposed on the Disclosing Party other than those expressly stated in this Agreement.\")])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groundtruths_rephrased[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rephrase into general questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Rephrasing queries: 100%|██████████| 60/60 [03:45<00:00,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rephrased groundtruth data to ../data/sample_benchmarks/contractnli_rephrased_split_3.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/hk/j9r7jggx4dxgt8gmzj_c2z080000gn/T/ipykernel_17242/441073255.py:95: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  tests = [gt.dict() for gt in groundtruths]\n"
     ]
    }
   ],
   "source": [
    "general_prompt = (\n",
    "    \"Example 1:\\n\"\n",
    "    \"Original: \\\"Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document indicate that the Agreement does not grant the Receiving Party any rights to the Confidential Information?\\\"\\n\"\n",
    "    \"Rephrased: \\\"Does a Non-Disclosure Agreement typically state that the Receiving Party has no rights to the Confidential Information shared under the agreement?\\\"\\n\\n\"\n",
    "    \n",
    "    \"Example 2:\\n\"\n",
    "    \"Original: \\\"Consider EFCA's Non-Disclosure Agreement; Does the document mention that some obligations of the Agreement may survive the termination of the Agreement?\\\"\\n\"\n",
    "    \"Rephrased: \\\"Do Non-Disclosure Agreements often include clauses specifying whether certain obligations continue even after the agreement is terminated?\\\"\\n\\n\"\n",
    "    \n",
    "    \"Example 3:\\n\"\n",
    "    \"Original: \\\"Consider the Data Use Agreement in New York City; Does the document specify whether the Receiving Party is required to destroy or return Confidential Information upon the termination of the Agreement?\\\"\\n\"\n",
    "    \"Rephrased: \\\"Is it common for Data Use Agreements to require the Receiving Party to destroy or return Confidential Information when the agreement ends?\\\"\\n\\n\"\n",
    "    \n",
    "    \"Now, rephrase the following question into a more GENERAL query, removing specific references to particular agreements while keeping the essence of the legal or contractual inquiry:\\n\"\n",
    "    \"Original: \\\"{question}\\\"\\n\\n\"\n",
    "    \"Rephrased: \"\n",
    ")\n",
    "\n",
    "# Load the ground truth test data from a JSON file.\n",
    "groundtruths_sample3 = f\"../data/{'sample_' if _use_sample else ''}benchmarks/{dataset_name}_split_3.json\"\n",
    "groundtruths = load_groundtruth(groundtruths_sample3)\n",
    "\n",
    "# Load a text-to-text generation pipeline using an open-source model.\n",
    "model = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "\n",
    "# Rephrase a specified percentage (e.g., 50%) of the queries.\n",
    "rephrased_groundtruths = rephrase_groundtruth_queries(groundtruths, model, percentage=1, custom_prompt=general_prompt)\n",
    "\n",
    "# Save the updated groundtruth test data.\n",
    "rephrased_file = f\"../data/{'sample_' if _use_sample else ''}benchmarks/{dataset_name}_rephrased_split_3.json\"\n",
    "save_groundtruth(rephrased_groundtruths, rephrased_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "query='\"Do the terms of Grindrod SA\\'s Non-Disclosure Agreement state that Confidential Information shall only include technical information?\"' snippets=[QASnippet(file_path='contractnli/Grindrod%20SA%20Confidentiality%20and%20Non-Disclosure%20Undertaking.txt', span=(757, 1492), answer='1.1 “Confidential Information” means; all technical, commercial, procurement requirements, purchasing, manufacturing, customer lists, investors, employees, business and contractual relationships, business forecasts, sales and merchandising, and marketing plans business or personnel information disclosed or otherwise made available in any format and/or physical manner by Grindrod SA or becoming available, before, during and/or after the execution of an interaction, duty or obligation including all information that makes itself known to the Vendor or comes into being as a result of the rendering, production and/or delivery of an agreement/understanding/request for quotation/contract or Purchase Order, or any other interaction. ')]\n",
      "query='\"Does a non-disclosure agreement typically state that the receiving party has no rights to the Confidential Information shared under the agreement?\"' snippets=[QASnippet(file_path='contractnli/CopAcc_NDA-and-ToP-Mentors_2.0_2017.txt', span=(11461, 11963), answer='Any and all proprietary rights, including but not limited to rights to and in inventions, patent rights, utility models, copyrights, trademarks and trade secrets, in and to any Confidential Information shall be and remain with the Participants respectively, and Mentor shall not have any right, license, title or interest in or to any Confidential Information, except the limited right to review, assess and help develop such Confidential Information in connection with the Copernicus Accelerator 2017.')]\n",
      "query='\"Does DBT\\'s Mutual Non-Disclosure Agreement restrict the use of Confidential Information to the purposes stated in the Agreement??\"' snippets=[QASnippet(file_path='contractnli/DBT%20Mutual%20NDA.txt', span=(2723, 2947), answer='Each party agrees that it shall not make use of, disseminate, or in any way disclose any Confidential Information of the Disclosing Party to any person, firm, or business, except to the extent necessary for the Transaction. ')]\n",
      "query='\"The Non-Disclosure Agreement between IGC and LSE does not include a clause that prevents the Receiving Party from disclosing the fact that the Agreement was agreed upon or negotiated?\"' snippets=[QASnippet(file_path='contractnli/IGC-Non-Disclosure-Agreement-LSE-Sample.txt', span=(2636, 3052), answer=\"Confidential Information means all confidential information (however recorded, preserved or disclosed) disclosed by a party or its Representatives to the other party and that party's Representatives including but not limited to:\\n(a) The fact that discussions and negotiations are taking place concerning the Purpose and the status of those discussions and negotiations;\\n(b) The existence and terms of this Agreement;\"), QASnippet(file_path='contractnli/IGC-Non-Disclosure-Agreement-LSE-Sample.txt', span=(8385, 9060), answer=\"No party shall make, or permit any person to make, any public announcement concerning this Agreement, the Purpose or its prospective interest in the Purpose without the prior written consent of the other party (such consent not to be unreasonably withheld or delayed) except as required by law or any governmental or regulatory authority (including, without limitation, any relevant securities exchange) or by any court or other authority of competent jurisdiction. No party shall make use of the other party's name or any information acquired through its dealings with the other party for publicity or marketing purposes without the prior written consent of the other party.\")]\n",
      "query='\"Does the confidentiality provision of the Confidential Information agreement in Euler Hermes allow the Receiving Party to share some Confidential Information with their employees?\"' snippets=[QASnippet(file_path='contractnli/eulerhermes-nda.txt', span=(1794, 2026), answer='1. Euler Hermes shall keep the Confidential Information confidential and may disclose the Confidential Information only to its Affiliates, employees, contractors, or consultants for the Purpose described above and no other purpose. ')]\n"
     ]
    }
   ],
   "source": [
    "groundtruths_rephrased = load_groundtruth(rephrased_file)\n",
    "\n",
    "print(len(groundtruths_rephrased))\n",
    "for i in groundtruths_rephrased[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import openai\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # This will load variables from .env into os.environ\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def rephrase_question_openai(question: str,\n",
    "                        custom_prompt: str,\n",
    "                        model: str = \"gpt-3.5-turbo\",\n",
    "                        max_tokens: int = 150,\n",
    "                        temperature: float = 0.8) -> str:\n",
    "    \"\"\"\n",
    "    Rephrases a question using the OpenAI API.\n",
    "    \n",
    "    The custom_prompt should contain a {question} placeholder that will be replaced with the original question.\n",
    "    \n",
    "    Example custom_prompt:\n",
    "    \"Example 1:\\n\"\n",
    "    \"Original: \\\"Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document indicate that the Agreement does not grant any rights to the Confidential Information?\\\"\\n\"\n",
    "    \"Rephrased: \\\"In the Non-Disclosure Agreement between CopAcc and ToP Mentors, does it explicitly state that the Receiving Party is not granted any rights to the Confidential Information?\\\"\\n\\n\"\n",
    "    \"Example 2:\\n\"\n",
    "    \"Original: \\\"Consider EFCA's Non-Disclosure Agreement; Does the document mention that some obligations of the Agreement may survive the termination of the Agreement?\\\"\\n\"\n",
    "    \"Rephrased: \\\"Does EFCA's Non-Disclosure Agreement mention whether certain obligations continue even after the Agreement is terminated?\\\"\\n\\n\"\n",
    "    \"Example 3:\\n\"\n",
    "    \"Original: \\\"Consider the Data Use Agreement in New York City; Does the document specify whether the Receiving Party is required to destroy or return Confidential Information upon the termination of the Agreement?\\\"\\n\"\n",
    "    \"Rephrased: \\\"In the Data Use Agreement for New York City, does the document specify if the Receiving Party must destroy or return Confidential Information once the Agreement ends?\\\"\\n\\n\"\n",
    "    \"Now, rephrase the following question in a natural, conversational style while preserving the two parts (the document details and the query):\\n\"\n",
    "    \"Original: \\\"{question}\\\"\\n\\n\"\n",
    "    \"Rephrased: \"\n",
    "    \n",
    "    The function expects the output to start with \"Rephrased:\" and then the rephrased text.\n",
    "    \"\"\"\n",
    "    prompt = custom_prompt.format(question=question)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that rephrases questions.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "    )\n",
    "    generated_text = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    \n",
    "    # Remove a \"Rephrased:\" prefix if present.\n",
    "    if generated_text.lower().startswith(\"rephrased:\"):\n",
    "        rephrased = generated_text[len(\"Rephrased:\"):].strip()\n",
    "    else:\n",
    "        rephrased = generated_text.strip()\n",
    "    return rephrased\n",
    "\n",
    "def rephrase_groundtruth_queries_openai(groundtruths: List[QAGroundTruth],\n",
    "                                 percentage: float = 0.5,\n",
    "                                 custom_prompt: Optional[str] = None,\n",
    "                                 model: str = \"gpt-3.5-turbo\",\n",
    "                                 max_tokens: int = 150,\n",
    "                                 temperature: float = 0.8) -> List[QAGroundTruth]:\n",
    "    \"\"\"\n",
    "    Rephrases a given percentage of queries in the ground truth using the OpenAI API.\n",
    "    \n",
    "    Args:\n",
    "      groundtruths: List of QAGroundTruth objects.\n",
    "      percentage: Fraction of queries to rephrase (e.g., 0.5 means 50%).\n",
    "      custom_prompt: A custom prompt string with a {question} placeholder.\n",
    "      model: The OpenAI model name.\n",
    "      max_tokens: Maximum tokens for the API call.\n",
    "      temperature: Sampling temperature.\n",
    "    \n",
    "    Returns:\n",
    "      The updated list of QAGroundTruth objects with rephrased queries.\n",
    "    \"\"\"\n",
    "    if custom_prompt is None:\n",
    "        custom_prompt = (\n",
    "            \"Example 1:\\n\"\n",
    "            \"Original: \\\"Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document indicate that the Agreement does not grant any rights to the Confidential Information?\\\"\\n\"\n",
    "            \"Rephrased: \\\"In the Non-Disclosure Agreement between CopAcc and ToP Mentors, does it explicitly state that the Receiving Party is not granted any rights to the Confidential Information?\\\"\\n\\n\"\n",
    "            \"Example 2:\\n\"\n",
    "            \"Original: \\\"Consider EFCA's Non-Disclosure Agreement; Does the document mention that some obligations of the Agreement may survive the termination of the Agreement?\\\"\\n\"\n",
    "            \"Rephrased: \\\"Does EFCA's Non-Disclosure Agreement mention whether certain obligations continue even after the Agreement is terminated?\\\"\\n\\n\"\n",
    "            \"Example 3:\\n\"\n",
    "            \"Original: \\\"Consider the Data Use Agreement in New York City; Does the document specify whether the Receiving Party is required to destroy or return Confidential Information upon the termination of the Agreement?\\\"\\n\"\n",
    "            \"Rephrased: \\\"In the Data Use Agreement for New York City, does the document specify if the Receiving Party must destroy or return Confidential Information once the Agreement ends?\\\"\\n\\n\"\n",
    "            \"Now, rephrase the following question in a natural, conversational style while preserving the two parts (the document details and the query):\\n\"\n",
    "            \"Original: \\\"{question}\\\"\\n\\n\"\n",
    "            \"Rephrased: \"\n",
    "        )\n",
    "    \n",
    "    num_to_rephrase = int(len(groundtruths) * percentage)\n",
    "    indices = random.sample(range(len(groundtruths)), num_to_rephrase)\n",
    "    \n",
    "    for idx in tqdm(indices, desc=\"Rephrasing queries\"):\n",
    "        original_query = groundtruths[idx].query\n",
    "        new_query = rephrase_question_openai(original_query, custom_prompt, model=model, max_tokens=max_tokens, temperature=temperature)\n",
    "        groundtruths[idx].query = new_query\n",
    "    return groundtruths\n",
    "\n",
    "def save_groundtruth(groundtruths: List[QAGroundTruth], output_path: str):\n",
    "    \"\"\"\n",
    "    Saves the list of QAGroundTruth objects to a JSON file in the expected format.\n",
    "    \"\"\"\n",
    "    tests = [gt.dict() for gt in groundtruths]\n",
    "    output_data = {\"tests\": tests}\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "    print(f\"Saved rephrased groundtruth data to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rephrasing queries: 100%|██████████| 60/60 [00:37<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rephrased groundtruth data to ../data/sample_benchmarks/contractnli_rephrased_split_3.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/hk/j9r7jggx4dxgt8gmzj_c2z080000gn/T/ipykernel_21960/2789246184.py:110: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  tests = [gt.dict() for gt in groundtruths]\n"
     ]
    }
   ],
   "source": [
    "general_prompt = (\n",
    "    \"Example 1:\\n\"\n",
    "    \"Original: \\\"Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document indicate that the Agreement does not grant the Receiving Party any rights to the Confidential Information?\\\"\\n\"\n",
    "    \"Rephrased: \\\"Does a Non-Disclosure Agreement typically state that the Receiving Party has no rights to the Confidential Information shared under the agreement?\\\"\\n\\n\"\n",
    "    \n",
    "    \"Example 2:\\n\"\n",
    "    \"Original: \\\"Consider EFCA's Non-Disclosure Agreement; Does the document mention that some obligations of the Agreement may survive the termination of the Agreement?\\\"\\n\"\n",
    "    \"Rephrased: \\\"Do Non-Disclosure Agreements often include clauses specifying whether certain obligations continue even after the agreement is terminated?\\\"\\n\\n\"\n",
    "    \n",
    "    \"Example 3:\\n\"\n",
    "    \"Original: \\\"Consider the Data Use Agreement in New York City; Does the document specify whether the Receiving Party is required to destroy or return Confidential Information upon the termination of the Agreement?\\\"\\n\"\n",
    "    \"Rephrased: \\\"Is it common for Data Use Agreements to require the Receiving Party to destroy or return Confidential Information when the agreement ends?\\\"\\n\\n\"\n",
    "    \n",
    "    \"Now, rephrase the following question into a more general query, minimising specific references to particular agreements while keeping the essence of the legal or contractual inquiry, don't use the same pattern as 'Is it common ..', keep the question vary:\\n\"\n",
    "    \"Original: \\\"{question}\\\"\\n\\n\"\n",
    "    \"Rephrased: \"\n",
    ")\n",
    "\n",
    "# Load the ground truth test data from a JSON file.\n",
    "groundtruths_sample3 = f\"../data/{'sample_' if _use_sample else ''}benchmarks/{dataset_name}_split_3.json\"\n",
    "groundtruths = load_groundtruth(groundtruths_sample3)\n",
    "\n",
    "# Rephrase a specified percentage (e.g., 50%) of the queries.\n",
    "rephrased_groundtruths = rephrase_groundtruth_queries_openai(groundtruths, model = \"gpt-3.5-turbo\", percentage=1, custom_prompt=general_prompt, max_tokens=200)\n",
    "\n",
    "# Save the updated groundtruth test data.\n",
    "rephrased_file = f\"../data/{'sample_' if _use_sample else ''}benchmarks/{dataset_name}_rephrased_split_3.json\"\n",
    "save_groundtruth(rephrased_groundtruths, rephrased_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "query='\"Within Non-Disclosure Agreements, is it typically specified that the definition of Confidential Information is limited to technical details?\"' snippets=[QASnippet(file_path='contractnli/Grindrod%20SA%20Confidentiality%20and%20Non-Disclosure%20Undertaking.txt', span=(757, 1492), answer='1.1 “Confidential Information” means; all technical, commercial, procurement requirements, purchasing, manufacturing, customer lists, investors, employees, business and contractual relationships, business forecasts, sales and merchandising, and marketing plans business or personnel information disclosed or otherwise made available in any format and/or physical manner by Grindrod SA or becoming available, before, during and/or after the execution of an interaction, duty or obligation including all information that makes itself known to the Vendor or comes into being as a result of the rendering, production and/or delivery of an agreement/understanding/request for quotation/contract or Purchase Order, or any other interaction. ')]\n",
      "query='\"Typically, do Non-Disclosure Agreements specify that the Receiving Party does not have rights to the Confidential Information shared under the agreement?\"' snippets=[QASnippet(file_path='contractnli/CopAcc_NDA-and-ToP-Mentors_2.0_2017.txt', span=(11461, 11963), answer='Any and all proprietary rights, including but not limited to rights to and in inventions, patent rights, utility models, copyrights, trademarks and trade secrets, in and to any Confidential Information shall be and remain with the Participants respectively, and Mentor shall not have any right, license, title or interest in or to any Confidential Information, except the limited right to review, assess and help develop such Confidential Information in connection with the Copernicus Accelerator 2017.')]\n",
      "query='\"Are Non-Disclosure Agreements typically designed to limit the use of Confidential Information to specific purposes outlined in the agreement?\"' snippets=[QASnippet(file_path='contractnli/DBT%20Mutual%20NDA.txt', span=(2723, 2947), answer='Each party agrees that it shall not make use of, disseminate, or in any way disclose any Confidential Information of the Disclosing Party to any person, firm, or business, except to the extent necessary for the Transaction. ')]\n",
      "query='\"Are Non-Disclosure Agreements typically designed to prevent the Receiving Party from disclosing the details of how the agreement was reached or negotiated?\"' snippets=[QASnippet(file_path='contractnli/IGC-Non-Disclosure-Agreement-LSE-Sample.txt', span=(2636, 3052), answer=\"Confidential Information means all confidential information (however recorded, preserved or disclosed) disclosed by a party or its Representatives to the other party and that party's Representatives including but not limited to:\\n(a) The fact that discussions and negotiations are taking place concerning the Purpose and the status of those discussions and negotiations;\\n(b) The existence and terms of this Agreement;\"), QASnippet(file_path='contractnli/IGC-Non-Disclosure-Agreement-LSE-Sample.txt', span=(8385, 9060), answer=\"No party shall make, or permit any person to make, any public announcement concerning this Agreement, the Purpose or its prospective interest in the Purpose without the prior written consent of the other party (such consent not to be unreasonably withheld or delayed) except as required by law or any governmental or regulatory authority (including, without limitation, any relevant securities exchange) or by any court or other authority of competent jurisdiction. No party shall make use of the other party's name or any information acquired through its dealings with the other party for publicity or marketing purposes without the prior written consent of the other party.\")]\n",
      "query='\"Are recipients typically permitted to disclose certain Confidential Information to their employees in Non-Disclosure Agreements?\"' snippets=[QASnippet(file_path='contractnli/eulerhermes-nda.txt', span=(1794, 2026), answer='1. Euler Hermes shall keep the Confidential Information confidential and may disclose the Confidential Information only to its Affiliates, employees, contractors, or consultants for the Purpose described above and no other purpose. ')]\n"
     ]
    }
   ],
   "source": [
    "groundtruths_rephrased = load_groundtruth(rephrased_file)\n",
    "\n",
    "print(len(groundtruths_rephrased))\n",
    "for i in groundtruths_rephrased[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample queries into 3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 194 queries.\n",
      "Saved 60 queries to ../data/sample_benchmarks/contractnli_split_1.json\n",
      "Saved 60 queries to ../data/sample_benchmarks/contractnli_split_2.json\n",
      "Saved 60 queries to ../data/sample_benchmarks/contractnli_split_3.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hk/j9r7jggx4dxgt8gmzj_c2z080000gn/T/ipykernel_17242/3158612484.py:8: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  tests = [gt.dict() for gt in groundtruths]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def save_groundtruth(groundtruths: List[QAGroundTruth], output_path: str):\n",
    "    \"\"\"\n",
    "    Saves a list of QAGroundTruth objects to a JSON file in the expected format.\n",
    "    \"\"\"\n",
    "    tests = [gt.dict() for gt in groundtruths]\n",
    "    output_data = {\"tests\": tests}\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "    print(f\"Saved {len(groundtruths)} queries to {output_path}\")\n",
    "\n",
    "\n",
    "groundtruths = load_groundtruth(test_file)\n",
    "print(f\"Loaded {len(groundtruths)} queries.\")\n",
    "\n",
    "# Shuffle the queries randomly.\n",
    "random.shuffle(groundtruths)\n",
    "\n",
    "# Split the list into 3 nearly equal parts.\n",
    "chunks = [groundtruths[i:i+60] for i in range(0, 180, 60)]\n",
    "\n",
    "# Save each chunk to a separate file.\n",
    "for i, chunk in enumerate(chunks, start=1):\n",
    "    splitted_file = f\"../data/{'sample_' if _use_sample else ''}benchmarks/{dataset_name}_split_{i}.json\"\n",
    "    save_groundtruth(chunk, splitted_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
